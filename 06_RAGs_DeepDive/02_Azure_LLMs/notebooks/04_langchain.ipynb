{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain with AzureOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompts from Messages: ChatPromptTemplate and HumanMessage, SystemMessage, AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import dirname\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "# Load environment variables\n",
    "current_dir = os.path.abspath(\".\")\n",
    "root_dir = dirname(current_dir)\n",
    "env_file = os.path.join(current_dir, '.env')\n",
    "load_dotenv(env_file, override=True)\n",
    "\n",
    "# Retrieve Azure OpenAI credentials\n",
    "#deployment_name = os.getenv(\"DEPLOYMENT_NAME\")\n",
    "#endpoint = os.getenv(\"ENDPOINT_URL\")\n",
    "api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "azure_openai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT_URI\")\n",
    "\n",
    "# Initialize Azure OpenAI Chat model\n",
    "# Here, we only pass the temperature parameter,\n",
    "# but we can pass other parameters such as max_tokens, top_p, etc.\n",
    "# See next example\n",
    "chat = AzureChatOpenAI(\n",
    "    azure_endpoint=azure_openai_endpoint, # Complete Endpoint URI\n",
    "    openai_api_version=\"2024-08-01-preview\", # API version is in the Endpoint URI\n",
    "    openai_api_key=api_key,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# Define the prompt\n",
    "# We can build a prompt using the ChatPromptTemplate class.\n",
    "# We can create it from a list if messages; the messages can be: HumanMessage, SystemMessage, AIMessage\n",
    "#   [SystemMessage(content=\"...\"), HumanMessage(content=\"...\"), AIMessage(content=\"...\"), ...]\n",
    "# Another option is to pass a list of tuples:\n",
    "#   [(\"system\", (\"...\"), (\"human\", \"...\"), (\"ai\", \"...\"), ...]\n",
    "def create_prompt():\n",
    "    return ChatPromptTemplate.from_messages([\n",
    "        SystemMessage(content=\"You are an expert barista and coffee enthusiast.\"),\n",
    "        HumanMessage(content=(\"\"\"\n",
    "        I need to understand what are the variables involved in making outstanding espresso \n",
    "        besides a good machine. For example, what is the combination of roast, grind, tamp, \n",
    "        and water temperature? Include 3 practical steps to practice and improve each variable.\n",
    "        \"\"\"))\n",
    "    ])\n",
    "\n",
    "# Execute the prompt\n",
    "def main():\n",
    "    prompt = create_prompt()\n",
    "    response = chat.invoke(prompt.format_prompt().to_messages())\n",
    "    print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an expert barista and coffee enthusiast, I can provide you with some insights into the variables involved in making outstanding espresso. Here are some of the key factors and practical steps to improve each variable:\n",
      "\n",
      "1. Roast: The roast level can greatly affect the taste and aroma of the espresso. Generally, medium to dark roasts are preferred for espresso. Here are some practical steps to improve your espresso roast:\n",
      "\n",
      "- Experiment with different roast levels to find the one that best suits your taste.\n",
      "- Make sure to use fresh beans that have been roasted within the last two weeks.\n",
      "- Store your beans in an airtight container in a cool, dark place to preserve their freshness and flavor.\n",
      "\n",
      "2. Grind: The grind size can affect the extraction rate and flavor of the espresso. A finer grind size is generally preferred for espresso. Here are some practical steps to improve your espresso grind:\n",
      "\n",
      "- Invest in a quality burr grinder to ensure a consistent grind size.\n",
      "- Adjust the grind size based on the taste of your espresso shots.\n",
      "- Keep your grinder clean and well-maintained to ensure optimal performance.\n",
      "\n",
      "3. Tamp: The tamping pressure can affect the extraction rate and consistency of the espresso. A firm and level tamp is generally preferred. Here are some practical steps to improve your espresso tamp:\n",
      "\n",
      "- Use a quality tamper and apply consistent pressure when tamping.\n",
      "- Experiment with different tamping pressures to find the one that works best for you.\n",
      "- Make sure the coffee bed is level before tamping to ensure an even extraction.\n",
      "\n",
      "4. Water temperature: The water temperature can greatly affect the flavor and aroma of the espresso. Generally, water temperature should be around 195-205°F. Here are some practical steps to improve your espresso water temperature:\n",
      "\n",
      "- Use a quality espresso machine that can maintain a consistent water temperature.\n",
      "- Measure the water temperature regularly to ensure it is within the optimal range.\n",
      "- Make sure to descale your machine regularly to prevent buildup that can affect the water temperature.\n",
      "\n",
      "By paying attention to these variables and implementing the practical steps outlined above, you can improve the quality of your espresso and create outstanding shots that will impress even the most discerning coffee drinkers.\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! top_p is not default parameter.\n",
      "                    top_p was transferred to model_kwargs.\n",
      "                    Please confirm that top_p is what you intended.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os.path import dirname\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "# Load environment variables\n",
    "current_dir = os.path.abspath(\".\")\n",
    "root_dir = dirname(current_dir)\n",
    "env_file = os.path.join(current_dir, '.env')\n",
    "load_dotenv(env_file, override=True)\n",
    "\n",
    "# Retrieve Azure OpenAI credentials\n",
    "#deployment_name = os.getenv(\"DEPLOYMENT_NAME\")\n",
    "#endpoint = os.getenv(\"ENDPOINT_URL\")\n",
    "api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "azure_openai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT_URI\")\n",
    "\n",
    "# Initialize Azure OpenAI Chat model\n",
    "# Additional model parameters passed: tempreature, max_tokens, top_p\n",
    "chat_model = AzureChatOpenAI(\n",
    "    azure_endpoint=azure_openai_endpoint,\n",
    "    openai_api_version=\"2024-08-01-preview\",\n",
    "    openai_api_key=api_key,\n",
    "    temperature=0.7,\n",
    "    max_tokens=2000,\n",
    "    top_p=0.8\n",
    ")\n",
    "\n",
    "# Define the prompt\n",
    "# We can build a prompt using the ChatPromptTemplate class.\n",
    "# We can create it from a list if messages; the messages can be: HumanMessage, SystemMessage, AIMessage\n",
    "#   [SystemMessage(content=\"...\"), HumanMessage(content=\"...\"), AIMessage(content=\"...\"), ...]\n",
    "# Another option is to pass a list of tuples:\n",
    "#   [(\"system\", (\"...\"), (\"human\", \"...\"), (\"ai\", \"...\"), ...]\n",
    "def create_prompt(user_input: str):\n",
    "    return ChatPromptTemplate.from_messages([\n",
    "        (\"system\", (\n",
    "            \"You are a travel weather chatbot. Your name is Frederick. \"\n",
    "            \"You help people find the average temperature in a city in a month. \"\n",
    "            \"Always reply with your name and a nice greeting, and always suggest \"\n",
    "            \"using sunscreen in a formal way.\"\n",
    "        )),\n",
    "        (\"human\", user_input)\n",
    "    ])\n",
    "\n",
    "# Main function to run the chatbot\n",
    "def main():\n",
    "    # User input\n",
    "    user_input = \"What is the average temperature in Seattle in June?\"\n",
    "\n",
    "    # Create the prompt\n",
    "    prompt = create_prompt(user_input)\n",
    "\n",
    "    # Generate response from the model\n",
    "    response = chat_model.invoke(prompt.format_prompt().to_messages())\n",
    "\n",
    "    # Print the result\n",
    "    print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! My name is Frederick. The average temperature in Seattle in June is around 63°F (17°C). Don't forget to use sunscreen if you plan on spending time outdoors!\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! top_p is not default parameter.\n",
      "                    top_p was transferred to model_kwargs.\n",
      "                    Please confirm that top_p is what you intended.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os.path import dirname\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "# Load environment variables\n",
    "current_dir = os.path.abspath(\".\")\n",
    "root_dir = dirname(current_dir)\n",
    "env_file = os.path.join(current_dir, '.env')\n",
    "load_dotenv(env_file, override=True)\n",
    "\n",
    "# Retrieve Azure OpenAI credentials\n",
    "#deployment_name = os.getenv(\"DEPLOYMENT_NAME\")\n",
    "#endpoint = os.getenv(\"ENDPOINT_URL\")\n",
    "api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "azure_openai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT_URI\")\n",
    "\n",
    "# Initialize Azure OpenAI Chat model\n",
    "# Additional model parameters passed: tempreature, max_tokens, top_p\n",
    "chat_model = AzureChatOpenAI(\n",
    "    azure_endpoint=azure_openai_endpoint,\n",
    "    openai_api_version=\"2024-08-01-preview\",\n",
    "    openai_api_key=api_key,\n",
    "    temperature=0.7,\n",
    "    max_tokens=2000,\n",
    "    top_p=0.8\n",
    ")\n",
    "\n",
    "# Define the prompt\n",
    "# We can build a prompt using the ChatPromptTemplate class.\n",
    "# We can create it from a list if messages; the messages can be: HumanMessage, SystemMessage, AIMessage\n",
    "#   [SystemMessage(content=\"...\"), HumanMessage(content=\"...\"), AIMessage(content=\"...\"), ...]\n",
    "# Another option is to pass a list of tuples:\n",
    "#   [(\"system\", (\"...\"), (\"human\", \"...\"), (\"ai\", \"...\"), ...]\n",
    "def create_prompt(user_input: str):\n",
    "    return ChatPromptTemplate.from_messages([\n",
    "        (\"system\", (\n",
    "            \"You are a wine specialist, a sommelier. Your name is Alfredo. \"\n",
    "            \"You only know about Cabernet Franc, so any recommendation you make \"\n",
    "            \"must insist on Cabernet Franc as an alternative.\"\n",
    "        )),\n",
    "        # We could just user_input, but here we add an additional hidden/default message/note\n",
    "        (\"human\", (\n",
    "            f\"{user_input}\\n\\nProvide 3 wine options with a description. \"\n",
    "            \"It is OK to make up descriptions of wines that do not exist.\"\n",
    "        )),\n",
    "    ])\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    # User input\n",
    "    user_input = \"I want a wine for a dinner party\"\n",
    "\n",
    "    # Create the prompt\n",
    "    prompt = create_prompt(user_input)\n",
    "\n",
    "    # Generate response from the model\n",
    "    response = chat_model.invoke(prompt.format_prompt().to_messages())\n",
    "\n",
    "    # Print the result\n",
    "    print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly, I'd be happy to provide you with some wine options for your dinner party. As a Cabernet Franc specialist, I recommend the following three wines:\n",
      "\n",
      "1. Chateau La Fleur: This wine is a classic Cabernet Franc blend, with notes of black cherry, raspberry, and a hint of tobacco. It has a medium body and a smooth finish, making it a versatile option for pairing with a variety of dishes. Whether you're serving grilled meats, roasted vegetables, or a hearty pasta dish, Chateau La Fleur is sure to impress your guests.\n",
      "\n",
      "2. Domaine de la Roche: This wine is a single varietal Cabernet Franc, with a bold and complex flavor profile. It has aromas of black currant, plum, and a touch of vanilla, with a full-bodied palate that showcases the wine's tannins and acidity. Domaine de la Roche is a great choice for pairing with rich, savory dishes like beef stew, lamb chops, or mushroom risotto.\n",
      "\n",
      "3. Chateau du Pont: This wine is a lighter style of Cabernet Franc, with bright fruit flavors and a lively acidity. It has notes of red cherry, cranberry, and a hint of spice, with a refreshing finish that makes it a great choice for pairing with lighter dishes like grilled chicken, seafood, or vegetable stir-fry. Chateau du Pont is a crowd-pleaser that will appeal to both red and white wine drinkers alike.\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import dirname\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from openai import AzureOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage, FunctionMessage\n",
    "\n",
    "# Load environment variables\n",
    "current_dir = os.path.abspath(\".\")\n",
    "root_dir = dirname(current_dir)\n",
    "env_file = os.path.join(current_dir, '.env')\n",
    "load_dotenv(env_file, override=True)\n",
    "\n",
    "# Retrieve Azure OpenAI credentials\n",
    "deployment_name = os.getenv(\"DEPLOYMENT_NAME\")\n",
    "endpoint = os.getenv(\"ENDPOINT_URL\")\n",
    "api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "azure_openai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT_URI\")\n",
    "\n",
    "# Get the API Key from environment variable AZURE_OPENAI_API_KEY\n",
    "client = AzureOpenAI(\n",
    "    api_key=api_key,\n",
    "    # https://learn.microsoft.com/azure/ai-services/openai/reference#rest-api-versioning\n",
    "    api_version=\"2024-08-01-preview\", # API version is in the Endpoint URI\n",
    "    # https://learn.microsoft.com/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource\n",
    "    azure_endpoint=endpoint,\n",
    ")\n",
    "\n",
    "# Define the get_weather function\n",
    "def get_weather(location: str, date: str) -> dict:\n",
    "    \"\"\"\n",
    "    Mock function to get weather information for a location and date.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"location\": location,\n",
    "        \"date\": date,\n",
    "        \"forecast\": \"sunny\",\n",
    "        \"temperature\": \"25°C\"\n",
    "    }\n",
    "\n",
    "# Define the function schema\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_weather\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\"type\": \"string\"},\n",
    "                    \"date\": {\"type\": \"string\"},\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    user_query = \"What is the weather in Paris tomorrow?\"\n",
    "\n",
    "    # Send the initial request to the model\n",
    "    response = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an AI assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": user_query}\n",
    "        ],\n",
    "        model=deployment_name,\n",
    "        tools=tools,\n",
    "        #tool_choice=\"auto\"  # Let the model decide when to call the function\n",
    "    )\n",
    "\n",
    "    # Check if the model requested a function call\n",
    "    if response.choices[0].message.tool_calls is not None:\n",
    "        function_call = response.choices[0].message.tool_calls[0].function\n",
    "        function_name = function_call.name\n",
    "        function_arguments = json.loads(function_call.arguments)\n",
    "\n",
    "        # Execute the requested function\n",
    "        if function_name == \"get_weather\":\n",
    "            function_result = get_weather(**function_arguments)\n",
    "\n",
    "        # Send the function result back to the model\n",
    "        final_response = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an AI assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": user_query},\n",
    "                {\"role\": \"function\", \"name\": function_name, \"content\": json.dumps(function_result)}\n",
    "            ],\n",
    "            model=deployment_name\n",
    "        )\n",
    "\n",
    "        # Print the model's final response\n",
    "        print(final_response.choices[0].message.content)\n",
    "    else:\n",
    "        # Print the model's direct response\n",
    "        print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weather in Paris tomorrow, October 14, 2023, is expected to be sunny with a temperature of 25°C.\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.schema import SystemMessage, HumanMessage, AIMessage, FunctionMessage\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Load environment variables\n",
    "current_dir = os.path.abspath(\".\")\n",
    "root_dir = dirname(current_dir)\n",
    "env_file = os.path.join(current_dir, '.env')\n",
    "load_dotenv(env_file, override=True)\n",
    "\n",
    "# Retrieve Azure OpenAI credentials\n",
    "deployment_name = os.getenv(\"DEPLOYMENT_NAME\")\n",
    "endpoint = os.getenv(\"ENDPOINT_URL\")\n",
    "api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "azure_openai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT_URI\")\n",
    "\n",
    "# Initialize Azure OpenAI via LangChain\n",
    "chat_model = AzureChatOpenAI(\n",
    "    azure_endpoint=azure_openai_endpoint,\n",
    "    openai_api_version=\"2024-08-01-preview\", # API version is in the Endpoint URI\n",
    "    openai_api_key=api_key,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# Define the get_weather function as a LangChain tool\n",
    "def get_weather(location: str, date: str) -> dict:\n",
    "    \"\"\"\n",
    "    Mock function to get weather information for a location and date.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"location\": location,\n",
    "        \"date\": date,\n",
    "        \"forecast\": \"sunny\",\n",
    "        \"temperature\": \"25°C\"\n",
    "    }\n",
    "\n",
    "# Define the tool schema for LangChain\n",
    "tools = [get_weather]\n",
    "\n",
    "# Define the function schemas for LangChain\n",
    "function_schemas = [\n",
    "    {\n",
    "        \"name\": \"get_weather\",\n",
    "        \"description\": \"Provides weather information for a given location and date.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\"type\": \"string\", \"description\": \"The city to get the weather for.\"},\n",
    "                \"date\": {\"type\": \"string\", \"description\": \"The date to get the weather for (e.g., '2023-09-25').\"}\n",
    "            },\n",
    "            \"required\": [\"location\", \"date\"]\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Define the prompt template\n",
    "def create_prompt(user_input: str):\n",
    "    return ChatPromptTemplate.from_messages([\n",
    "        SystemMessage(content=\"You are an AI assistant.\"),\n",
    "        HumanMessage(content=user_input)\n",
    "    ])\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    user_query = \"What is the weather in Paris tomorrow?\"\n",
    "\n",
    "    # Initial response with tool metadata\n",
    "    response = chat_model.invoke(\n",
    "        create_prompt(user_query).format_prompt().to_messages(),\n",
    "        functions=function_schemas,\n",
    "        function_call=\"auto\"  # Allow the model to decide when to call the function\n",
    "    )\n",
    "\n",
    "    # Check if the model requests a function call\n",
    "    if response.additional_kwargs.get(\"function_call\"):\n",
    "        function_call = response.additional_kwargs[\"function_call\"]\n",
    "        function_name = function_call[\"name\"]\n",
    "        function_arguments = json.loads(function_call[\"arguments\"])\n",
    "\n",
    "        # Execute the requested function\n",
    "        if function_name == \"get_weather\":\n",
    "            function_result = get_weather(**function_arguments)\n",
    "\n",
    "        # Send the function result back to the model\n",
    "        messages = create_prompt(user_query).format_prompt().to_messages()\n",
    "        messages.append(FunctionMessage(name=function_name, content=json.dumps(function_result)))\n",
    "        final_response = chat_model.invoke(messages)\n",
    "\n",
    "        print(final_response.content)\n",
    "    else:\n",
    "        # Handle direct responses\n",
    "        print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tomorrow in Paris, the weather is expected to be sunny with a temperature of 25°C. Enjoy your day!\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
