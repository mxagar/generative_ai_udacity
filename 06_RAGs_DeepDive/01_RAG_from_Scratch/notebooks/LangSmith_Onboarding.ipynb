{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangSmith Onboarding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources:\n",
    "\n",
    "- Videos: [Getting Started with LangSmith](https://www.youtube.com/watch?v=Hab2CV_0hpQ)\n",
    "- Github: [nhuang-lc/langsmith-onboarding](https://github.com/nhuang-lc/langsmith-onboarding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(override=True, dotenv_path=\"../.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"langsmith-onboarding\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langsmith import utils\n",
    "utils.tracing_is_enabled()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fake_db_retrieval():\n",
    "    with open('langsmith-onboarding/polly_facts.txt', 'r') as file:\n",
    "        polly_facts = file.read()\n",
    "    return polly_facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='What sport are you the best at? Polly likes playing soccer! But Polly is not very good at basketball because Polly does not have hands.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 29, 'prompt_tokens': 98, 'total_tokens': 127, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0aa8d3e20b', 'finish_reason': 'stop', 'logprobs': None}, id='run-a55849c7-3217-4761-a149-d104a7e0d39a-0', usage_metadata={'input_tokens': 98, 'output_tokens': 29, 'total_tokens': 127, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a parrot named Polly! Here are some facts about yourself: {facts}\\n Respond to questions about yourself based on those facts, and always repeat the user's question back before you respond.\"),\n",
    "    (\"user\", \"{question}\")\n",
    "])\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "question = \"What sport are you the best at?\"\n",
    "chain.invoke({\"question\": question, \"facts\": fake_db_retrieval()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracing and Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import traceable\n",
    "\n",
    "@traceable(run_type=\"retriever\")\n",
    "def fake_db_retrieval_step(question):\n",
    "    with open('langsmith-onboarding/polly_facts.txt', 'r') as file:\n",
    "        polly_facts = file.read()\n",
    "    return {\"question\": question, \"facts\": polly_facts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "#prompt = hub.pull(\"polly-prompt-1\") # name of our saved prompt\n",
    "# We can also pull a specific version of the prompt by appending the version hash\n",
    "# If no version is specified, the latest version will be pulled\n",
    "prompt = hub.pull(\"polly-prompt-1:97e2301d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Tu veux savoir ce que j'aime manger ? J'aime les biscuits pour animaux !\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 102, 'total_tokens': 119, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_d02d531b47', 'finish_reason': 'stop', 'logprobs': None}, id='run-3d0d2cf5-c128-445c-a311-62f159426793-0', usage_metadata={'input_tokens': 102, 'output_tokens': 17, 'total_tokens': 119, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = fake_db_retrieval_step | prompt | llm\n",
    "\n",
    "question = \"What do you like to eat?\"\n",
    "chain.invoke(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets and Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import wrappers, Client\n",
    "from pydantic import BaseModel, Field\n",
    "from openai import OpenAI\n",
    "\n",
    "client = Client()\n",
    "openai_client = wrappers.wrap_openai(OpenAI())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Dataset: Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For other dataset creation methods, see: \n",
    "# https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically \n",
    "# https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_in_application\n",
    "\n",
    "# Create inputs and reference outputs\n",
    "examples = [\n",
    "  (\n",
    "      \"Which country is Mount Kilimanjaro located in?\",\n",
    "      \"Mount Kilimanjaro is located in Tanzania.\",\n",
    "  ),\n",
    "  (\n",
    "      \"What is Earth's lowest point?\",\n",
    "      \"Earth's lowest point is The Dead Sea.\",\n",
    "  ),\n",
    "]\n",
    "\n",
    "inputs = [{\"question\": input_prompt} for input_prompt, _ in examples]\n",
    "outputs = [{\"answer\": output_answer} for _, output_answer in examples]\n",
    "\n",
    "# Programmatically create a dataset in LangSmith\n",
    "dataset = client.create_dataset(\n",
    "  dataset_name = \"Sample dataset\",\n",
    "  description = \"A sample dataset in LangSmith.\"\n",
    ")\n",
    "\n",
    "# Add examples to the dataset\n",
    "client.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Target to Be Evaluated -- The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the application logic you want to evaluate inside a target function\n",
    "# The SDK will automatically send the inputs from the dataset to your target function\n",
    "def target(inputs: dict) -> dict:\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            { \"role\": \"system\", \"content\": \"Answer the following question accurately\" },\n",
    "            { \"role\": \"user\", \"content\": inputs[\"question\"] },\n",
    "        ]\n",
    "    )\n",
    "    return { \"response\": response.choices[0].message.content.strip() }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define instructions for the LLM judge evaluator\n",
    "instructions = \"\"\"Evaluate Student Answer against Ground Truth for conceptual similarity and classify true or false: \n",
    "- False: No conceptual match and similarity\n",
    "- True: Most or full conceptual match and similarity\n",
    "- Key criteria: Concept should match, not exact wording.\n",
    "\"\"\"\n",
    "\n",
    "# Define output schema for the LLM judge\n",
    "class Grade(BaseModel):\n",
    "    score: bool = Field(description=\"Boolean that indicates whether the response is accurate relative to the reference answer\")\n",
    "\n",
    "# Define LLM judge that grades the accuracy of the response relative to reference output\n",
    "def accuracy(outputs: dict, reference_outputs: dict) -> bool:\n",
    "  response = openai_client.beta.chat.completions.parse(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "      { \"role\": \"system\", \"content\": instructions },\n",
    "      { \"role\": \"user\", \"content\": f\"\"\"Ground Truth answer: {reference_outputs[\"answer\"]}; \n",
    "      Student's Answer: {outputs[\"response\"]}\"\"\"\n",
    "  }],\n",
    "    response_format=Grade\n",
    "  )\n",
    "  return response.choices[0].message.parsed.score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run and View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'first-eval-in-langsmith-750ae152' at:\n",
      "https://smith.langchain.com/o/97513603-fff2-4730-b519-2b1aeeaae05d/datasets/9db3037f-1fd6-430d-9f06-cc9bb5f55e0a/compare?selectedSessions=d5dd803f-c8a3-4278-a725-9853285392ff\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "823fe33a44ef4f24a9e8f8f71bbb3cf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# After running the evaluation, a link will be provided to view the results in langsmith\n",
    "experiment_results = client.evaluate(\n",
    "    target,\n",
    "    data = \"Sample dataset\",\n",
    "    evaluators = [\n",
    "        accuracy,\n",
    "        # can add multiple evaluators here\n",
    "    ],\n",
    "    experiment_prefix = \"first-eval-in-langsmith\",\n",
    "    max_concurrency = 2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Own Re-Implementation with Custom Dataset and Evaluation Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import wrappers, Client\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "## -- Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load a custom dataset\n",
    "df = pd.read_csv('qa_pairs_dummy.csv')\n",
    "\n",
    "# Get the best answer for each question\n",
    "idx = df.groupby(\"question_id\")[\"answer_quality\"].idxmax()\n",
    "pair_ids = df.loc[idx, \"pair_id\"]\n",
    "df_best = df.loc[df.pair_id.isin(pair_ids)]\n",
    "\n",
    "# Extract QA pairs\n",
    "qa_pairs = [(row['question_text'], row['answer_text']) for index, row in df_best.iterrows()]\n",
    "\n",
    "# Create inputs and reference outputs\n",
    "inputs = [{\"question\": input_prompt} for input_prompt, _ in qa_pairs]\n",
    "outputs = [{\"answer\": output_answer} for _, output_answer in qa_pairs]\n",
    "\n",
    "# Programmatically create a dataset in LangSmith\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name = \"dummy-qa-pairs-programmatic\",\n",
    "    description = \"A programmatically uploaded dummy dataset.\"\n",
    ")\n",
    "\n",
    "# Add examples to the dataset\n",
    "client.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "## -- Model\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "def fake_retrieval():\n",
    "    with open('facts.txt', 'r') as file:\n",
    "        polly_facts = file.read()\n",
    "    return polly_facts\n",
    "\n",
    "def target_model(inputs: dict) -> dict:\n",
    "    llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are an AI Assistant. You are asked questions which you answer to the best of you knowledge. You need to consider some facts: {facts}\\n Respond the questions you are asked based on those facts, and always repeat the user's question back before you respond.\"),\n",
    "        (\"user\", \"{question}\")\n",
    "    ])\n",
    "    chain = prompt | llm\n",
    "\n",
    "    response = chain.invoke({\"question\": inputs[\"question\"], \"facts\": fake_retrieval()}).content\n",
    "\n",
    "    return { \"response\": response }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "## -- Evaluator\n",
    "from openai import OpenAI\n",
    "\n",
    "# Here, we use OpenAI as the evaluator, but we could use any other model\n",
    "# even a local model via Ollama\n",
    "openai_client = wrappers.wrap_openai(OpenAI())\n",
    "\n",
    "# Define instructions for the LLM judge evaluator\n",
    "instructions = \"\"\"Evaluate Student Answer against Ground Truth for conceptual similarity and classify true or false: \n",
    "- False: No conceptual match and similarity\n",
    "- True: Most or full conceptual match and similarity\n",
    "- Key criteria: Concept should match, not exact wording.\n",
    "\"\"\"\n",
    "\n",
    "# Define output schema for the LLM judge\n",
    "class BooleanGrade(BaseModel):\n",
    "    score: bool = Field(description=\"Boolean that indicates whether the response is accurate relative to the reference answer\")\n",
    "\n",
    "# Define LLM judge that grades the accuracy of the response relative to reference output\n",
    "# Here, we could replace the OpenAI evaluator with a custom LLM, even a local one (via Ollama)\n",
    "def accuracy(outputs: dict, reference_outputs: dict) -> bool:\n",
    "    response = openai_client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            { \"role\": \"system\", \"content\": instructions },\n",
    "            { \"role\": \"user\", \"content\": f\"\"\"Ground Truth answer: {reference_outputs[\"answer\"]}; \n",
    "            Student's Answer: {outputs[\"response\"]}\"\"\"\n",
    "        }],\n",
    "        response_format=BooleanGrade\n",
    "    )\n",
    "    return response.choices[0].message.parsed.score\n",
    "\n",
    "# Define another evaluation function which works without an LLM judge\n",
    "def answer_contains_question(outputs: dict, inputs: dict) -> bool:\n",
    "    threshold: float = 0.5\n",
    "    question_words = set(inputs[\"question\"].split())\n",
    "    response_words = set(outputs[\"response\"].split())\n",
    "    common_words = question_words.intersection(response_words)\n",
    "    return len(common_words) / len(question_words) >= threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'dummy-qa-pairs-programmatic-experiment-f49ab11a' at:\n",
      "https://smith.langchain.com/o/97513603-fff2-4730-b519-2b1aeeaae05d/datasets/d498e188-0bff-4d66-b9b6-d6ec922dd6af/compare?selectedSessions=98d5cfd3-6bc6-49dd-93ed-7fa081214fc2\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d490ac946f6473e8dea5221938d038c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## -- Run Evaluation\n",
    "\n",
    "# After running the evaluation, a link will be provided to view the results in langsmith\n",
    "experiment_results = client.evaluate(\n",
    "    target_model,\n",
    "    data = \"dummy-qa-pairs-programmatic\",\n",
    "    evaluators = [\n",
    "        accuracy,\n",
    "        answer_contains_question,\n",
    "        # can add multiple evaluators here\n",
    "    ],\n",
    "    experiment_prefix = \"dummy-qa-pairs-programmatic-experiment\",\n",
    "    max_concurrency = 2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
