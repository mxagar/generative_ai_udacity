# Generative AI and Machine Learning Podcasts

Table of contents:

- [Generative AI and Machine Learning Podcasts](#generative-ai-and-machine-learning-podcasts)
  - [Latent Space Podcast](#latent-space-podcast)
    - [Thomas Scialom from Meta/FAIR - Llama 2, 3 \& 4: Synthetic Data, RLHF, Agents on the path to Open Source AGI](#thomas-scialom-from-metafair---llama-2-3--4-synthetic-data-rlhf-agents-on-the-path-to-open-source-agi)
      - [**Multilinguality**](#multilinguality)
      - [**Chinchilla**](#chinchilla)
      - [**Distillation**](#distillation)
      - [**Tokenizers**](#tokenizers)
      - [**Agents for Training**](#agents-for-training)
      - [**Reinforcement Learning with Human Feedback (RLHF)**](#reinforcement-learning-with-human-feedback-rlhf)
      - [**ELO and Other Performance Benchmarks**](#elo-and-other-performance-benchmarks)
      - [**Toolformer**](#toolformer)
      - [**JEPA/GEPA**](#jepagepa)
      - [**Gaia and World Models**](#gaia-and-world-models)
      - [**Thinking in Latent Space**](#thinking-in-latent-space)
    - [Clémentine Fourier from HuggingFace - Benchmarks 201: Why Leaderboards \> Arenas \>\> LLM-as-Judge](#clémentine-fourier-from-huggingface---benchmarks-201-why-leaderboards--arenas--llm-as-judge)
      - [**Hugging Face Leaderboard**](#hugging-face-leaderboard)
      - [**AI Advancement Requires More or New Benchmarks**](#ai-advancement-requires-more-or-new-benchmarks)
      - [**LLMs as a Judge Are Bad; What to Use Instead**](#llms-as-a-judge-are-bad-what-to-use-instead)
      - [**Vibe Checks**](#vibe-checks)
      - [**Wisdom of the Crowd**](#wisdom-of-the-crowd)
      - [**Cohere**](#cohere)
      - [**Models Are Biased, As Humans**](#models-are-biased-as-humans)
      - [**Lack of Diversity in Annotations**](#lack-of-diversity-in-annotations)
      - [**EF Evaluations: Unit Tests for LLMs**](#ef-evaluations-unit-tests-for-llms)
      - [**MMLU**](#mmlu)
      - [**GPQA**](#gpqa)
      - [**GAIA: Benchmark for Agents**](#gaia-benchmark-for-agents)
      - [**HF Eval Is Free, They Have a Cluster of Powerful GPUs**](#hf-eval-is-free-they-have-a-cluster-of-powerful-gpus)
      - [**Model Calibration: Correlation of Log Probability with Truth; Confidence Intervals on How Sure the Model Is**](#model-calibration-correlation-of-log-probability-with-truth-confidence-intervals-on-how-sure-the-model-is)
      - [**Robustness in Prompting: 10 Variations of the Same Prompt Should Yield Similar Answers**](#robustness-in-prompting-10-variations-of-the-same-prompt-should-yield-similar-answers)
      - [**Factuality and Psychofancy: Avoid Echo Chambers, Some Things Are Facts**](#factuality-and-psychofancy-avoid-echo-chambers-some-things-are-facts)


## Latent Space Podcast

### Thomas Scialom from Meta/FAIR - Llama 2, 3 & 4: Synthetic Data, RLHF, Agents on the path to Open Source AGI

[Thomas Scialom from Meta/FAIR - Llama 2, 3 & 4: Synthetic Data, RLHF, Agents on the path to Open Source AGI](https://www.latent.space/p/llama-3)

JUL 23, 2024

#### **Multilinguality**
   - **Emergence in Training:** Multilinguality was initially a significant focus, especially in projects like Galactica and Bloom. The surprising finding was that multilingual capabilities emerged naturally with very little data, which was not expected during the early phases of research.
   - **Llama 3 Enhancements:** In Llama 3, multilinguality is emphasized further by tailoring pre-training to include a diverse mix of languages. This was done by collecting higher-quality human annotations in non-English languages and continuing to pre-train on a data mix that was 90% multilingual tokens.

#### **Chinchilla**
   - **Chinchilla Trap:** The interview discusses the "Chinchilla trap," referring to the balance between model size and the amount of training data. Chinchilla scaling laws emphasize that more training tokens per model weight lead to optimal performance. However, this approach is ideal for achieving the best benchmarks but not necessarily for models intended for widespread use. For inference efficiency, it’s better to overtrain models even if it means not achieving the highest possible benchmark score, as this can produce more practical and usable models.

#### **Distillation**
   - **Distillation Strategy:** Llama 3 and its smaller versions rely heavily on distillation from larger models, particularly Llama 3’s 405B model. There is speculation that the smaller models (8B and 70B) were distilled from the larger 405B model. Distillation also involves using synthetic data generated from earlier versions of Llama models to enhance the training of newer ones.
   - **Synthetic Data:** Llama 3 post-training was primarily based on synthetic data generated by Llama 2, which suggests that distillation didn’t involve new human-written answers but leveraged the existing high-quality outputs from previous models.

#### **Tokenizers**
   - **Importance of Tokenizer Size:** The tokenizer used in Llama models expanded significantly from Llama 2 to Llama 3, increasing from 34,000 to 128,000 tokens. This expansion allows the model to represent more concepts, understand nuances better, and handle longer contexts with fewer tokens. A larger tokenizer reduces the number of tokens needed to process the same amount of text, which effectively extends the context size and improves the model's efficiency.
   - **Impact on Model Performance:** The choice to increase the tokenizer size is seen as crucial for optimizing the balance between compute resources and model performance.

#### **Agents for Training**
   - **Future of Agents in Llama 4:** The discussion on agents points towards their importance in the future development of Llama models, particularly Llama 4. The focus will be on improving the model's ability to plan and execute tasks autonomously, closing the "gap of intelligence" observed in current models that struggle with agentic workflows. This includes work on tools like Toolformer and GAIA, which help the models in complex, multi-step reasoning tasks.
   - **Tool Integration:** Current models are limited in their ability to use tools effectively without prompting techniques or external frameworks, but future iterations aim to integrate agentic capabilities more deeply.

#### **Reinforcement Learning with Human Feedback (RLHF)**
   - **RLHF vs. Supervised Fine-Tuning:** The interviewee emphasizes that RLHF is more effective than supervised fine-tuning for improving model performance. While supervised fine-tuning involves human-written prompts and answers, RLHF leverages human preferences between generated outputs to iteratively improve the model.
   - **Scaling RLHF:** During Llama 2’s development, they found that models trained with RLHF often outperformed even human-annotated answers. This led to the realization that human feedback is better at judging quality than producing content from scratch. For Llama 3, synthetic data generated by Llama 2 was used for RLHF, indicating that even for new models, older versions can effectively bootstrap the process.
   - **Improvement Areas:** RLHF is seen as particularly effective in improving areas like empathy in model responses, even surpassing human benchmarks in some cases. The interviewee also discusses how RLHF can be scaled and applied across different domains, such as coding, reasoning, and multilinguality, to enhance overall model performance. 

#### **ELO and Other Performance Benchmarks**
   - **ELO Score in Evaluations:** The ELO score is used as a metric to evaluate model performance in competitive environments, like the Arena leaderboard where models are pitted against each other in head-to-head comparisons. For Llama 3, particularly the 7TB model, the team was pleasantly surprised by how well it performed in these evaluations, even though they didn’t initially expect such strong results.
   - **Human and Model Evaluations:** The interviewee mentions the difficulty in evaluating advanced models because they are becoming increasingly good, making it harder to find prompts that challenge them. A diverse set of benchmarks, including both traditional ones like MMLU and newer metrics like model-as-a-judge and human evaluation, are necessary to capture the full range of capabilities.
   - **Challenge of Overfitting:** A key concern is that models might overfit to specific benchmarks, which can make these benchmarks less useful over time. Therefore, a diverse and evolving set of evaluation metrics is essential to ensure that models genuinely improve across a broad spectrum of tasks.

#### **Toolformer**
   - **Toolformer Integration:** Toolformer is a key aspect of enhancing Llama models with tool usage capabilities. It allows the model to interface with external tools like calculators, search engines, or code execution environments, which significantly boosts its problem-solving abilities beyond what’s possible with just the model’s internal weights.
   - **Llama 3’s Capabilities:** Llama 3 is trained to handle tool usage from day one, making it state-of-the-art in this area. It can perform zero-shot function calling and handle complex tasks that require multiple steps or interaction with external systems. This integration is seen as a step towards more advanced agentic behavior.
   - **Potential for Community Involvement:** The interviewee encourages the open-source community to further fine-tune Llama 3 for tool usage, as this area holds significant potential for expanding the model's utility in real-world applications.

#### **JEPA/GEPA**
   - **Fundamental Research:** JEPA (Joint Embedding Predictive Architecture) and GEPA (General Embedding Predictive Architecture) are described as more fundamental research projects. While they share a common goal with the Llama models—advancing AI capabilities—they represent a different line of inquiry focused on foundational aspects of AI architecture.
   - **Not Directly Integrated:** The work on JEPA/GEPA is not directly integrated into the current Llama models but is seen as part of the broader research landscape that could influence future developments.

#### **Gaia and World Models**
   - **Gaia General Assistant Benchmark:** Gaia represents a significant step in developing agents that can perform complex tasks by following instructions over multiple steps. The Gaia benchmark tests these capabilities by evaluating models on tasks that require planning, backtracking, and tool usage.
   - **Performance of LLMs as Agents:** Initial results showed a stark difference in performance between earlier models (like GPT-3.5) and more advanced ones (like GPT-4) on Gaia, indicating a "gap of intelligence" that needs to be bridged to achieve more autonomous and effective agents.
   - **Future Directions:** With Llama 3 and the work leading into Llama 4, there is a focus on enhancing these agentic capabilities. This includes improving models' ability to plan, execute, and correct actions autonomously, which is crucial for developing more advanced AI agents.

#### **Thinking in Latent Space**
   - **Complex Planning and Reasoning:** The concept of "thinking in latent space" relates to the idea that advanced models could eventually perform complex reasoning and planning directly within the model's internal representations (latent space) rather than relying heavily on external step-by-step processes.
   - **Future AI Capabilities:** The interviewee expresses hope that future AI models, such as Llama 4, will move towards being able to take a task, process it internally, and return an answer directly without needing to explicitly simulate each step. This represents a shift towards more integrated and efficient problem-solving within the model itself, reducing the need for external prompts or frameworks.

### Clémentine Fourier from HuggingFace - Benchmarks 201: Why Leaderboards > Arenas >> LLM-as-Judge

[Clémentine Fourier from HuggingFace - Benchmarks 201: Why Leaderboards > Arenas >> LLM-as-Judge](https://www.latent.space/p/benchmarks-201)

JUL 12, 2024

Here’s a summary of what Clémentine Fourrier discusses regarding the specified topics in the podcast:

#### **Hugging Face Leaderboard**
   - **Purpose and Impact:** The Hugging Face OpenLLM Leaderboard is a critical tool for evaluating and comparing LLMs, offering a reproducible way to assess model performance across various benchmarks. The leaderboard is widely used by the community, with thousands of models evaluated and millions of visitors. It serves as a key resource for cutting through marketing claims and providing objective assessments.

#### **AI Advancement Requires More or New Benchmarks**
   - **Need for Updated Benchmarks:** As AI models have become increasingly advanced, existing benchmarks have started to plateau, making them less useful for distinguishing between models. New benchmarks, such as MMLU-Pro and GPQA, have been introduced to provide more challenging evaluations. The rapid pace of AI progress necessitates continuous updates to benchmarks to keep them relevant.

#### **LLMs as a Judge Are Bad; What to Use Instead**
   - **Problems with LLMs as Judges:** Using LLMs as judges introduces biases, such as preferring outputs from the same model family or the first answer provided (positional bias). This can lead to self-reinforcement and mode collapse, where models become too similar. Instead, Clémentine suggests using open-source models like Prometheus or JudgeLM for rankings, not scores, to avoid these issues.

#### **Vibe Checks**
   - **Importance of Vibe Checks:** Vibe checks are informal evaluations where users test models on specific use cases to see which one fits their needs best. These are considered necessary because general benchmarks might not fully capture a model’s performance on a user’s particular tasks. Vibe checks provide a practical, user-focused approach to model evaluation.

#### **Wisdom of the Crowd**
   - **Limitations in AI Evaluation:** The wisdom of the crowd approach, used in arenas like LMSys, can be effective for certain tasks but is limited when it comes to subjective evaluations. While this method works well for quantifiable tasks, it struggles with subjective or qualitative assessments, making it less reliable for rigorous AI evaluation.

#### **Cohere**
   - **Model Training Approach:** Cohere is highlighted for its distinct approach to model training, focusing on high-quality data and avoiding the use of outputs from other models to prevent bias. This results in models that are less likely to exhibit the homogenized behavior seen in models trained with shared datasets.

#### **Models Are Biased, As Humans**
   - **Inherent Bias in Models:** Clémentine discusses how models, like humans, can exhibit biases, particularly in evaluations where they prefer responses that align with user opinions (sycophancy). This bias is problematic, especially in factual or critical tasks, where accuracy should be prioritized over user preference.

#### **Lack of Diversity in Annotations**
   - **Annotation Bias Issues:** There is a concern about the lack of diversity among annotators, particularly in popular evaluation platforms like LMSys Arena. The majority of annotators tend to come from specific demographics (e.g., men from the US), which can skew the results and limit the generalizability of the findings.

#### **EF Evaluations: Unit Tests for LLMs**
   - **Structured Evaluation Method:** EF (Instruction Following Evaluation) is compared to unit tests in software development, focusing strictly on whether a model can follow instructions accurately. This method is praised for its lack of ambiguity and its straightforward evaluation criteria, making it a reliable benchmark for assessing LLMs' instruction-following capabilities.

#### **MMLU**
   - **Popular Benchmark:** MMLU (Massive Multitask Language Understanding) is a widely used benchmark that has become less effective as models have reached high levels of performance. The introduction of MMLU-Pro aims to address the saturation problem by offering more challenging tasks that better differentiate between advanced models.

#### **GPQA**
   - **Advanced Q&A Benchmark:** GPQA (Google-Proof Q&A) is highlighted as a more challenging version of benchmarks like MMLU, featuring questions that require specialized knowledge, often at a PhD level. This benchmark is designed to push the limits of model understanding and reasoning.

#### **GAIA: Benchmark for Agents**
   - **Real-World Agent Evaluation:** GAIA is a benchmark designed to test AI agents in real-world tasks, focusing on practical capabilities like web browsing and information extraction. Unlike other benchmarks that rely on artificial environments, GAIA evaluates agents in more realistic settings, providing a better measure of their practical utility.

#### **HF Eval Is Free, They Have a Cluster of Powerful GPUs**
   - **Compute Resources:** The Hugging Face evaluation system is free for the community, running on a shared cluster of powerful GPUs. The leaderboard jobs are prioritized lower than other research projects at Hugging Face, utilizing spare compute cycles, which allows the service to be provided at no cost to users.

#### **Model Calibration: Correlation of Log Probability with Truth; Confidence Intervals on How Sure the Model Is**
   - **Importance of Calibration:** Model calibration refers to the correlation between a model's confidence (log probability) and the correctness of its answers. Well-calibrated models can provide confidence intervals, indicating how certain they are about their outputs. This is important for applications where understanding the reliability of a model's answer is crucial.

#### **Robustness in Prompting: 10 Variations of the Same Prompt Should Yield Similar Answers**
   - **Consistency in Responses:** Robustness in prompting is the idea that slight variations in how a question is phrased should not lead to significantly different answers from the model. This consistency is important for ensuring that models are reliable and not overly sensitive to minor changes in input.

#### **Factuality and Psychofancy: Avoid Echo Chambers, Some Things Are Facts**
   - **Ensuring Model Objectivity:** Clémentine emphasizes the need for models to maintain factual accuracy and avoid reinforcing users' incorrect beliefs (psychofancy). It is crucial that models assertively present factual information, especially on topics where there is a clear, objective truth, to prevent the creation of echo chambers and misinformation. 


