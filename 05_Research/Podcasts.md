# Generative AI and Machine Learning Podcasts

Table of contents:

- [Generative AI and Machine Learning Podcasts](#generative-ai-and-machine-learning-podcasts)
  - [Latent Space Podcast](#latent-space-podcast)
    - [Thomas Scialom from Meta/FAIR - Llama 2, 3 \& 4: Synthetic Data, RLHF, Agents on the path to Open Source AGI](#thomas-scialom-from-metafair---llama-2-3--4-synthetic-data-rlhf-agents-on-the-path-to-open-source-agi)
    - [Clémentine Fourier from HuggingFace - Benchmarks 201: Why Leaderboards \> Arenas \>\> LLM-as-Judge](#clémentine-fourier-from-huggingface---benchmarks-201-why-leaderboards--arenas--llm-as-judge)
    - [Jerry Liu from LlamaIndex - RAG Is A Hack](#jerry-liu-from-llamaindex---rag-is-a-hack)


## Latent Space Podcast

### Thomas Scialom from Meta/FAIR - Llama 2, 3 & 4: Synthetic Data, RLHF, Agents on the path to Open Source AGI

[Thomas Scialom from Meta/FAIR - Llama 2, 3 & 4: Synthetic Data, RLHF, Agents on the path to Open Source AGI](https://www.latent.space/p/llama-3)

JUL 23, 2024

**Multilinguality**
 - **Emergence in Training:** Multilinguality was initially a significant focus, especially in projects like Galactica and Bloom. The surprising finding was that multilingual capabilities emerged naturally with very little data, which was not expected during the early phases of research.
 - **Llama 3 Enhancements:** In Llama 3, multilinguality is emphasized further by tailoring pre-training to include a diverse mix of languages. This was done by collecting higher-quality human annotations in non-English languages and continuing to pre-train on a data mix that was 90% multilingual tokens.

**Chinchilla**
- **Chinchilla Trap:** The interview discusses the "Chinchilla trap," referring to the balance between model size and the amount of training data. Chinchilla scaling laws emphasize that more training tokens per model weight lead to optimal performance. However, this approach is ideal for achieving the best benchmarks but not necessarily for models intended for widespread use. For inference efficiency, it’s better to overtrain models even if it means not achieving the highest possible benchmark score, as this can produce more practical and usable models.

**Distillation**
- **Distillation Strategy:** Llama 3 and its smaller versions rely heavily on distillation from larger models, particularly Llama 3’s 405B model. There is speculation that the smaller models (8B and 70B) were distilled from the larger 405B model. Distillation also involves using synthetic data generated from earlier versions of Llama models to enhance the training of newer ones.
- **Synthetic Data:** Llama 3 post-training was primarily based on synthetic data generated by Llama 2, which suggests that distillation didn’t involve new human-written answers but leveraged the existing high-quality outputs from previous models.

**Tokenizers**
- **Importance of Tokenizer Size:** The tokenizer used in Llama models expanded significantly from Llama 2 to Llama 3, increasing from 34,000 to 128,000 tokens. This expansion allows the model to represent more concepts, understand nuances better, and handle longer contexts with fewer tokens. A larger tokenizer reduces the number of tokens needed to process the same amount of text, which effectively extends the context size and improves the model's efficiency.
- **Impact on Model Performance:** The choice to increase the tokenizer size is seen as crucial for optimizing the balance between compute resources and model performance.

**Agents for Training**
- **Future of Agents in Llama 4:** The discussion on agents points towards their importance in the future development of Llama models, particularly Llama 4. The focus will be on improving the model's ability to plan and execute tasks autonomously, closing the "gap of intelligence" observed in current models that struggle with agentic workflows. This includes work on tools like Toolformer and GAIA, which help the models in complex, multi-step reasoning tasks.
- **Tool Integration:** Current models are limited in their ability to use tools effectively without prompting techniques or external frameworks, but future iterations aim to integrate agentic capabilities more deeply.

**Reinforcement Learning with Human Feedback (RLHF)**
- **RLHF vs. Supervised Fine-Tuning:** The interviewee emphasizes that RLHF is more effective than supervised fine-tuning for improving model performance. While supervised fine-tuning involves human-written prompts and answers, RLHF leverages human preferences between generated outputs to iteratively improve the model.
- **Scaling RLHF:** During Llama 2’s development, they found that models trained with RLHF often outperformed even human-annotated answers. This led to the realization that human feedback is better at judging quality than producing content from scratch. For Llama 3, synthetic data generated by Llama 2 was used for RLHF, indicating that even for new models, older versions can effectively bootstrap the process.
- **Improvement Areas:** RLHF is seen as particularly effective in improving areas like empathy in model responses, even surpassing human benchmarks in some cases. The interviewee also discusses how RLHF can be scaled and applied across different domains, such as coding, reasoning, and multilinguality, to enhance overall model performance. 

**ELO and Other Performance Benchmarks**
- **ELO Score in Evaluations:** The ELO score is used as a metric to evaluate model performance in competitive environments, like the Arena leaderboard where models are pitted against each other in head-to-head comparisons. For Llama 3, particularly the 7TB model, the team was pleasantly surprised by how well it performed in these evaluations, even though they didn’t initially expect such strong results.
- **Human and Model Evaluations:** The interviewee mentions the difficulty in evaluating advanced models because they are becoming increasingly good, making it harder to find prompts that challenge them. A diverse set of benchmarks, including both traditional ones like MMLU and newer metrics like model-as-a-judge and human evaluation, are necessary to capture the full range of capabilities.
- **Challenge of Overfitting:** A key concern is that models might overfit to specific benchmarks, which can make these benchmarks less useful over time. Therefore, a diverse and evolving set of evaluation metrics is essential to ensure that models genuinely improve across a broad spectrum of tasks.

**Toolformer**
- **Toolformer Integration:** Toolformer is a key aspect of enhancing Llama models with tool usage capabilities. It allows the model to interface with external tools like calculators, search engines, or code execution environments, which significantly boosts its problem-solving abilities beyond what’s possible with just the model’s internal weights.
- **Llama 3’s Capabilities:** Llama 3 is trained to handle tool usage from day one, making it state-of-the-art in this area. It can perform zero-shot function calling and handle complex tasks that require multiple steps or interaction with external systems. This integration is seen as a step towards more advanced agentic behavior.
- **Potential for Community Involvement:** The interviewee encourages the open-source community to further fine-tune Llama 3 for tool usage, as this area holds significant potential for expanding the model's utility in real-world applications.

**JEPA/GEPA**
- **Fundamental Research:** JEPA (Joint Embedding Predictive Architecture) and GEPA (General Embedding Predictive Architecture) are described as more fundamental research projects. While they share a common goal with the Llama models—advancing AI capabilities—they represent a different line of inquiry focused on foundational aspects of AI architecture.
- **Not Directly Integrated:** The work on JEPA/GEPA is not directly integrated into the current Llama models but is seen as part of the broader research landscape that could influence future developments.

**Gaia and World Models**
- **Gaia General Assistant Benchmark:** Gaia represents a significant step in developing agents that can perform complex tasks by following instructions over multiple steps. The Gaia benchmark tests these capabilities by evaluating models on tasks that require planning, backtracking, and tool usage.
- **Performance of LLMs as Agents:** Initial results showed a stark difference in performance between earlier models (like GPT-3.5) and more advanced ones (like GPT-4) on Gaia, indicating a "gap of intelligence" that needs to be bridged to achieve more autonomous and effective agents.
- **Future Directions:** With Llama 3 and the work leading into Llama 4, there is a focus on enhancing these agentic capabilities. This includes improving models' ability to plan, execute, and correct actions autonomously, which is crucial for developing more advanced AI agents.

**Thinking in Latent Space**
- **Complex Planning and Reasoning:** The concept of "thinking in latent space" relates to the idea that advanced models could eventually perform complex reasoning and planning directly within the model's internal representations (latent space) rather than relying heavily on external step-by-step processes.
- **Future AI Capabilities:** The interviewee expresses hope that future AI models, such as Llama 4, will move towards being able to take a task, process it internally, and return an answer directly without needing to explicitly simulate each step. This represents a shift towards more integrated and efficient problem-solving within the model itself, reducing the need for external prompts or frameworks.

### Clémentine Fourier from HuggingFace - Benchmarks 201: Why Leaderboards > Arenas >> LLM-as-Judge

[Clémentine Fourier from HuggingFace - Benchmarks 201: Why Leaderboards > Arenas >> LLM-as-Judge](https://www.latent.space/p/benchmarks-201)

JUL 12, 2024

Here’s a summary of what Clémentine Fourrier discusses regarding the specified topics in the podcast:

**Hugging Face Leaderboard**
- **Purpose and Impact:** The Hugging Face OpenLLM Leaderboard is a critical tool for evaluating and comparing LLMs, offering a reproducible way to assess model performance across various benchmarks. The leaderboard is widely used by the community, with thousands of models evaluated and millions of visitors. It serves as a key resource for cutting through marketing claims and providing objective assessments.

**AI Advancement Requires More or New Benchmarks**
- **Need for Updated Benchmarks:** As AI models have become increasingly advanced, existing benchmarks have started to plateau, making them less useful for distinguishing between models. New benchmarks, such as MMLU-Pro and GPQA, have been introduced to provide more challenging evaluations. The rapid pace of AI progress necessitates continuous updates to benchmarks to keep them relevant.

**LLMs as a Judge Are Bad; What to Use Instead**
- **Problems with LLMs as Judges:** Using LLMs as judges introduces biases, such as preferring outputs from the same model family or the first answer provided (positional bias). This can lead to self-reinforcement and mode collapse, where models become too similar. Instead, Clémentine suggests using open-source models like Prometheus or JudgeLM for rankings, not scores, to avoid these issues.

**Vibe Checks**
- **Importance of Vibe Checks:** Vibe checks are informal evaluations where users test models on specific use cases to see which one fits their needs best. These are considered necessary because general benchmarks might not fully capture a model’s performance on a user’s particular tasks. Vibe checks provide a practical, user-focused approach to model evaluation.

**Wisdom of the Crowd**
- **Limitations in AI Evaluation:** The wisdom of the crowd approach, used in arenas like LMSys, can be effective for certain tasks but is limited when it comes to subjective evaluations. While this method works well for quantifiable tasks, it struggles with subjective or qualitative assessments, making it less reliable for rigorous AI evaluation.

**Cohere**
- **Model Training Approach:** Cohere is highlighted for its distinct approach to model training, focusing on high-quality data and avoiding the use of outputs from other models to prevent bias. This results in models that are less likely to exhibit the homogenized behavior seen in models trained with shared datasets.

**Models Are Biased, As Humans**
- **Inherent Bias in Models:** Clémentine discusses how models, like humans, can exhibit biases, particularly in evaluations where they prefer responses that align with user opinions (sycophancy). This bias is problematic, especially in factual or critical tasks, where accuracy should be prioritized over user preference.

**Lack of Diversity in Annotations**
- **Annotation Bias Issues:** There is a concern about the lack of diversity among annotators, particularly in popular evaluation platforms like LMSys Arena. The majority of annotators tend to come from specific demographics (e.g., men from the US), which can skew the results and limit the generalizability of the findings.

**EF Evaluations: Unit Tests for LLMs**
- **Structured Evaluation Method:** EF (Instruction Following Evaluation) is compared to unit tests in software development, focusing strictly on whether a model can follow instructions accurately. This method is praised for its lack of ambiguity and its straightforward evaluation criteria, making it a reliable benchmark for assessing LLMs' instruction-following capabilities.

**MMLU**
- **Popular Benchmark:** MMLU (Massive Multitask Language Understanding) is a widely used benchmark that has become less effective as models have reached high levels of performance. The introduction of MMLU-Pro aims to address the saturation problem by offering more challenging tasks that better differentiate between advanced models.

**GPQA**
- **Advanced Q&A Benchmark:** GPQA (Google-Proof Q&A) is highlighted as a more challenging version of benchmarks like MMLU, featuring questions that require specialized knowledge, often at a PhD level. This benchmark is designed to push the limits of model understanding and reasoning.

**GAIA: Benchmark for Agents**
- **Real-World Agent Evaluation:** GAIA is a benchmark designed to test AI agents in real-world tasks, focusing on practical capabilities like web browsing and information extraction. Unlike other benchmarks that rely on artificial environments, GAIA evaluates agents in more realistic settings, providing a better measure of their practical utility.

**HF Eval Is Free, They Have a Cluster of Powerful GPUs**
- **Compute Resources:** The Hugging Face evaluation system is free for the community, running on a shared cluster of powerful GPUs. The leaderboard jobs are prioritized lower than other research projects at Hugging Face, utilizing spare compute cycles, which allows the service to be provided at no cost to users.

**Model Calibration: Correlation of Log Probability with Truth; Confidence Intervals on How Sure the Model Is**
- **Importance of Calibration:** Model calibration refers to the correlation between a model's confidence (log probability) and the correctness of its answers. Well-calibrated models can provide confidence intervals, indicating how certain they are about their outputs. This is important for applications where understanding the reliability of a model's answer is crucial.

**Robustness in Prompting: 10 Variations of the Same Prompt Should Yield Similar Answers**
- **Consistency in Responses:** Robustness in prompting is the idea that slight variations in how a question is phrased should not lead to significantly different answers from the model. This consistency is important for ensuring that models are reliable and not overly sensitive to minor changes in input.

**Factuality and Psychofancy: Avoid Echo Chambers, Some Things Are Facts**
- **Ensuring Model Objectivity:** Clémentine emphasizes the need for models to maintain factual accuracy and avoid reinforcing users' incorrect beliefs (psychofancy). It is crucial that models assertively present factual information, especially on topics where there is a clear, objective truth, to prevent the creation of echo chambers and misinformation. 


### Jerry Liu from LlamaIndex - RAG Is A Hack

[Jerry Liu from LlamaIndex - RAG Is A Hack](https://www.latent.space/p/llamaindex)

OCT 05, 2023

**RecSys (Recommendation Systems)**
- **Experience at Quora:** Jerry Liu shares that his work at Quora involved recommendation systems (RecSys), particularly focusing on ranking based on user preferences. The algorithms were primarily driven by embeddings, where user and item embeddings were trained to maximize their similarity, thereby improving recommendation quality. This work was largely about optimizing user engagement metrics like time spent on the site.
- **Relation to RAG:** Although not directly related to RAG (Retrieval Augmented Generation), his experience in RecSys at Quora provided foundational knowledge in information retrieval, which is a key component in the development of LlamaIndex and other LLM applications.

**Multimodal Models, CLIP**
- **Initial Interest in Multimodal Models:** Jerry Liu mentions that before focusing on text-based systems like LlamaIndex, he had a strong interest in multimodal data, including video data. He initially considered starting a project in this domain before shifting his focus to text.
- **CLIP and Multimodal Models:** Liu acknowledges the powerful properties of multimodal models like CLIP, which combine different types of data, such as text and images, into joint embeddings. He notes that while text became the primary focus for LlamaIndex, he recognizes the potential of multimodal models and how they can offer "mathematically nicer properties" by combining multiple embeddings. However, he emphasizes that text provides a modular and universal interface, which makes it particularly well-suited for the kind of applications LlamaIndex targets.

**RAG Risks:**
- **Risks if Context Windows Get Bigger:** Jerry Liu discusses the potential risk to RAG (Retrieval Augmented Generation) systems if context windows in language models become significantly larger. If models could handle much larger context windows, the need for complex retrieval mechanisms like RAG could diminish. However, he points out that even with larger context windows, the efficiency of RAG remains relevant, particularly in dealing with very large datasets (e.g., gigabytes to petabytes). He emphasizes that simply dumping vast amounts of data into a model’s context window would be inefficient due to network transfer costs and the sheer volume of information.
- **Risks if Companies Build Orchestration Frameworks:** Another potential risk to RAG is the development of comprehensive orchestration frameworks by large companies, which could integrate retrieval and fine-tuning directly into their models. This could reduce the necessity for third-party tools like LlamaIndex. Liu acknowledges this possibility but argues that there will always be room for RAG-like systems that offer flexibility, transparency, and modularity, especially for specific use cases where fine-grained control over data retrieval is important.
- **Risks if Fine-Tuning Gets Much Better/Easier:** Liu recognizes that if fine-tuning becomes significantly easier and more efficient, it could diminish the need for RAG. Fine-tuning allows models to internalize knowledge directly, potentially reducing the reliance on external retrieval mechanisms. However, he also notes that RAG offers advantages in terms of transparency, access control, and ease of use, which fine-tuning cannot fully replicate. As a result, he believes RAG will continue to be valuable, especially in scenarios where transparency and specific document sourcing are important.

**Current Context Window Sizes and Trends:**
- **Context Window Sizes:** Jerry Liu discusses the current standard context window sizes, mentioning that modern models typically operate with context windows of around 2,000 to 4,000 tokens. He explains that this range is generally sufficient for most retrieval and synthesis tasks, allowing for granular context while still fitting within the model’s processing capabilities.
- **Trends in Expanding Context Windows:** Liu acknowledges the trend towards expanding context windows, with some models now supporting up to 100,000 tokens. He suggests that while this expansion allows for more detailed and extensive context to be considered in a single model pass, there are still practical limitations. For example, larger context windows do not completely eliminate the need for retrieval mechanisms like RAG, especially when dealing with extremely large datasets or when specific, relevant information needs to be retrieved efficiently.

**When Should We Use RAG Compared to Fine-Tuning?**
- **Use Cases for RAG:** Jerry Liu suggests that RAG (Retrieval Augmented Generation) is particularly useful when you need transparency, modularity, and control over the data used in LLM outputs. RAG is beneficial for scenarios where you need to access and retrieve specific documents or data sources dynamically, without embedding all the knowledge into the model itself. It's also a good choice when fine-tuning is too expensive or complex.
- **When to Consider Fine-Tuning:** Fine-tuning, on the other hand, is better when you need the model to internalize knowledge, especially when you’re dealing with smaller, more specific datasets where performance improvements can be significant. Fine-tuning may also be more appropriate as it becomes easier and more efficient, potentially reducing the reliance on RAG for some use cases.

**How Much Does the LLM "Know" and How Much Comes from Retrieval?**
- **Balance Between Internal Knowledge and Retrieval:** Liu explains that in a RAG system, the LLM has a base of internalized knowledge from its training data, but the most up-to-date or specific information often comes from the retrieval mechanism. The retrieval system augments the model by pulling in relevant, context-specific information that the model might not "know" inherently or recall accurately from its training data.

**LLMOps vs MLOps**
- **Differences Between LLMOps and MLOps:** LLMOps (Large Language Model Operations) focuses on the operationalization of LLMs, including aspects like prompt engineering, retrieval optimization, and managing the interaction between LLMs and external data sources. MLOps (Machine Learning Operations), on the other hand, deals with the broader operational concerns of machine learning models, such as deployment, monitoring, and maintaining model performance over time. Liu emphasizes that while there are similarities, LLMOps requires specific considerations due to the unique nature of LLMs.

**Ablation Studies: What Are These?**
- **Definition and Purpose:** Ablation studies involve systematically removing or altering parts of a model or system to understand the impact of each component on overall performance. In the context of RAG, this could involve changing or removing specific retrieval strategies, components of the RAG pipeline, or even certain hyperparameters to see how each influences the final output.

**Hyperparameter Optimization for RAGs: Which Hyperparameters?**
- **Key Hyperparameters in RAG:** Liu mentions several hyperparameters that can be optimized in RAG systems, including the size of retrieval chunks, the similarity threshold for retrieval, and the type of retriever used (e.g., vector-based vs. keyword-based). Hyperparameter optimization in RAG is critical to balance performance and accuracy, especially when dealing with large-scale data or complex queries.

**How Can Retrieval Be Improved?**
- **Hierarchical Data:** Retrieval can be enhanced by structuring data hierarchically, allowing the system to drill down into more specific nodes of information as needed.
- **Hybrid Retrieval:** Combining different retrieval methods, such as vector-based and keyword-based retrieval, can improve accuracy and relevance.
- **Auto Retrieval:** Automating the retrieval process can make the system more efficient by dynamically adjusting the retrieval strategy based on the query.
- **Query Transformation:** Transforming the query before retrieval can help align it better with the available data, improving the chances of retrieving the most relevant information.
- **Chain of Thought Agent:** Creating an agent that retrieves different aspects of a query step by step allows for more complex, multi-part answers by building on retrieved information iteratively.

**SEC Insights: Full Stack LLM-Based Chatbot Which is a Template**
- **Overview of SEC Insights:** SEC Insights is a full-stack, LLM-based chatbot template that leverages LlamaIndex for retrieval and querying. It’s designed to demonstrate how to build a complete chatbot application using LLMs, including data ingestion, retrieval, and response generation. The template is meant to be a starting point for developers looking to create similar applications in different domains.

**Vector DBs: Structured (SQL) vs. Unstructured (Semantic) Data Querying**
- **Comparison of Querying Methods:** Liu discusses the differences between structured (SQL-based) and unstructured (semantic) data querying. Structured querying involves precise, predefined queries on tabular data, while unstructured querying allows for more flexible, semantic searches across diverse data types. In RAG systems, both methods can be important, depending on the use case.
- **PGVector and Weviate:** PGVector and Weviate are tools that enable semantic querying within databases, allowing for more nuanced and contextually relevant searches.
- **Role of Metadata:** Metadata plays a crucial role in improving retrieval by providing additional context and filtering capabilities, making searches more accurate and relevant.
- **Graph Representations:** Using graph representations can further enhance retrieval by capturing relationships between different pieces of data, enabling more complex queries and richer responses.

**RAG Evaluation: Component-Wise vs. End-to-End**
- **Component-Wise Evaluation:** Evaluating each component of a RAG system separately allows for a detailed understanding of how each part contributes to the overall performance.
- **End-to-End Evaluation:** Conversely, end-to-end evaluation assesses the entire RAG pipeline as a whole, which is important for understanding the user experience and final output quality.
- **Challenges with GPT-4 as Judge:** Liu mentions that using GPT-4 as a judge for evaluations can be unreliable due to inherent biases and limitations. He suggests that retrieval benchmarks are critical for more accurate assessment, and synthetic datasets can be useful in standardizing these evaluations.

**Agents as Reasoning Primitives**
- **Role of Agents:** Liu discusses how agents can be used as reasoning primitives in RAG systems, enabling more complex decision-making and multi-step reasoning processes. Agents can interact with multiple tools or data sources, retrieve and process information step by step, and synthesize more sophisticated outputs.

**The Role of Metadata for Improved Retrieval**
- **Importance of Metadata:** Metadata is essential for enhancing retrieval by adding layers of context and filtering capabilities. It allows the system to narrow down search results more effectively, improving both the speed and accuracy of the retrieval process. Metadata can include information like document type, author, date, or even semantic tags that provide deeper insight into the content.

