{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from helper import (\n",
    "    start_time,\n",
    "    time_since,\n",
    "    ShakespeareDataset,\n",
    "    TokenMapping,\n",
    "    build_model,\n",
    "    next_token,\n",
    "    # Character-based helpers\n",
    "    encode_text,\n",
    "    # Subword-based helpers\n",
    "    encode_text_from_tokenizer,\n",
    "    tokenize_text_from_tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Deterministic training\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Attempt GPU; if not, stay on CPU\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters in text file: 50,085\n"
     ]
    }
   ],
   "source": [
    "# Reduced data to make it manageable for smaller systems\n",
    "DATA_FILE: str = './data/shakespeare_small.txt'\n",
    "\n",
    "with open(DATA_FILE, 'r') as data_file:\n",
    "    raw_text = data_file.read()\n",
    "\n",
    "print(f'Number of characters in text file: {len(raw_text):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character-Based Text Generation\n",
    "\n",
    "The first model you'll build for text generation will use character-based\n",
    "tokens.\n",
    "\n",
    "Each token will be a single character from the text and the model will learn\n",
    "to predict the next character (a token).\n",
    "\n",
    "To generate text, the model will take in a new string,\n",
    "character-by-character, and then generate a new likely character based on the\n",
    "past input. Then the model will take into account that new character and\n",
    "generate the following character and so on and so on until the model has\n",
    "produced a set number of characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Text into Integer Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text: str) -> str:\n",
    "    # Normalize incoming text; can be multiple actions\n",
    "    # Just lowercase letters \n",
    "    normalized_text = text.lower()\n",
    "    # Keeping punctuation & other characters\n",
    "    return normalized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first citizen:\n",
      "before we proceed any further, hear me speak.\n",
      "\n",
      "all:\n",
      "speak, speak.\n",
      "\n",
      "first citizen:\n",
      "you are all resolved rather to die than to famish?\n",
      "\n",
      "all:\n",
      "resolved. resolved.\n",
      "\n",
      "first citizen:\n",
      "first, you know caius marcius is chief enemy to the people.\n",
      "\n",
      "all:\n",
      "we know't, we know't.\n",
      "\n",
      "first citizen:\n",
      "let us kill him, and we'll have corn at our own price.\n",
      "is't a verdict?\n",
      "\n",
      "all:\n",
      "no more talking on't; let it be done: away, away!\n",
      "\n",
      "second citizen:\n",
      "one word, good citizens.\n",
      "\n",
      "first citizen:\n",
      "we are accounted poor\n"
     ]
    }
   ],
   "source": [
    "# TEST: Is your text normalized the way you expected?\n",
    "# Only the first 500 characters of the original text\n",
    "normalized_text = normalize_text(raw_text[:500])\n",
    "print(normalized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretokenize_text(text: str) -> str | list[str]:\n",
    "    # Pretokenize normalized text into character strings\n",
    "    smaller_pieces = [char for char in text]\n",
    "    return smaller_pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['f', 'i', 'r', 's', 't', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'b', 'e', 'f', 'o', 'r', 'e', ' ', 'w', 'e', ' ', 'p', 'r', 'o', 'c', 'e', 'e', 'd', ' ', 'a', 'n', 'y', ' ', 'f', 'u', 'r', 't', 'h', 'e', 'r', ',', ' ', 'h', 'e', 'a', 'r', ' ', 'm', 'e', ' ', 's', 'p', 'e', 'a', 'k', '.', '\\n', '\\n', 'a', 'l', 'l', ':', '\\n', 's', 'p', 'e', 'a', 'k', ',', ' ', 's', 'p', 'e', 'a', 'k', '.', '\\n', '\\n', 'f', 'i', 'r', 's', 't', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'y', 'o', 'u', ' ', 'a', 'r', 'e', ' ', 'a', 'l', 'l', ' ', 'r', 'e', 's', 'o', 'l', 'v', 'e', 'd', ' ', 'r', 'a', 't', 'h', 'e', 'r', ' ', 't', 'o', ' ', 'd', 'i', 'e', ' ', 't', 'h', 'a', 'n', ' ', 't', 'o', ' ', 'f', 'a', 'm', 'i', 's', 'h', '?', '\\n', '\\n', 'a', 'l', 'l', ':', '\\n', 'r', 'e', 's', 'o', 'l', 'v', 'e', 'd', '.', ' ', 'r', 'e', 's', 'o', 'l', 'v', 'e', 'd', '.', '\\n', '\\n', 'f', 'i', 'r', 's', 't', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'f', 'i', 'r', 's', 't', ',', ' ', 'y', 'o', 'u', ' ', 'k', 'n', 'o', 'w', ' ', 'c', 'a', 'i', 'u', 's', ' ', 'm', 'a', 'r', 'c', 'i', 'u', 's', ' ', 'i', 's', ' ', 'c', 'h', 'i', 'e', 'f', ' ', 'e', 'n', 'e', 'm', 'y', ' ', 't', 'o', ' ', 't', 'h', 'e', ' ', 'p', 'e', 'o', 'p', 'l', 'e', '.', '\\n', '\\n', 'a', 'l', 'l', ':', '\\n', 'w', 'e', ' ', 'k', 'n', 'o', 'w', \"'\", 't', ',', ' ', 'w', 'e', ' ', 'k', 'n', 'o', 'w', \"'\", 't', '.', '\\n', '\\n', 'f', 'i', 'r', 's', 't', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'l', 'e', 't', ' ', 'u', 's', ' ', 'k', 'i', 'l', 'l', ' ', 'h', 'i', 'm', ',', ' ', 'a', 'n', 'd', ' ', 'w', 'e', \"'\", 'l', 'l', ' ', 'h', 'a', 'v', 'e', ' ', 'c', 'o', 'r', 'n', ' ', 'a', 't', ' ', 'o', 'u', 'r', ' ', 'o', 'w', 'n', ' ', 'p', 'r', 'i', 'c', 'e', '.', '\\n', 'i', 's', \"'\", 't', ' ', 'a', ' ', 'v', 'e', 'r', 'd', 'i', 'c', 't', '?', '\\n', '\\n', 'a', 'l', 'l', ':', '\\n', 'n', 'o', ' ', 'm', 'o', 'r', 'e', ' ', 't', 'a', 'l', 'k', 'i', 'n', 'g', ' ', 'o', 'n', \"'\", 't', ';', ' ', 'l', 'e', 't', ' ', 'i', 't', ' ', 'b', 'e', ' ', 'd', 'o', 'n', 'e', ':', ' ', 'a', 'w', 'a', 'y', ',', ' ', 'a', 'w', 'a', 'y', '!', '\\n', '\\n', 's', 'e', 'c', 'o', 'n', 'd', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'o', 'n', 'e', ' ', 'w', 'o', 'r', 'd', ',', ' ', 'g', 'o', 'o', 'd', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', 's', '.', '\\n', '\\n', 'f', 'i', 'r', 's', 't', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'w', 'e', ' ', 'a', 'r', 'e', ' ', 'a', 'c', 'c', 'o', 'u', 'n', 't', 'e', 'd', ' ', 'p', 'o', 'o', 'r']\n"
     ]
    }
   ],
   "source": [
    "# TEST: Is your (normalized) text pretokenized the way you expected?\n",
    "# Only the first 500 characters of the original text\n",
    "pretokenized_text = pretokenize_text(normalized_text)\n",
    "print(pretokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine normalization and pretokenization steps\n",
    "def tokenize_text(text: str) -> str | list[str]:\n",
    "    normalized_text: str = normalize_text(text)\n",
    "    pretokenized_text: str | list[str] = pretokenize_text(normalized_text)\n",
    "    # Characters are already tokens so pretokenized text is already tokenized\n",
    "    tokenized_text = pretokenized_text\n",
    "    return tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['f', 'i', 'r', 's', 't', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'b', 'e', 'f', 'o', 'r', 'e', ' ', 'w', 'e', ' ', 'p', 'r', 'o', 'c', 'e', 'e', 'd', ' ', 'a', 'n', 'y', ' ', 'f', 'u', 'r', 't', 'h', 'e', 'r', ',', ' ', 'h', 'e', 'a', 'r', ' ', 'm', 'e', ' ', 's', 'p', 'e', 'a', 'k', '.', '\\n', '\\n', 'a', 'l', 'l', ':', '\\n', 's', 'p', 'e', 'a', 'k', ',', ' ', 's', 'p', 'e', 'a', 'k', '.', '\\n', '\\n', 'f', 'i', 'r', 's', 't', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'y', 'o', 'u', ' ', 'a', 'r', 'e', ' ', 'a', 'l', 'l', ' ', 'r', 'e', 's', 'o', 'l', 'v', 'e', 'd', ' ', 'r', 'a', 't', 'h', 'e', 'r', ' ', 't', 'o', ' ', 'd', 'i', 'e', ' ', 't', 'h', 'a', 'n', ' ', 't', 'o', ' ', 'f', 'a', 'm', 'i', 's', 'h', '?', '\\n', '\\n', 'a', 'l', 'l', ':', '\\n', 'r', 'e', 's', 'o', 'l', 'v', 'e', 'd', '.', ' ', 'r', 'e', 's', 'o', 'l', 'v', 'e', 'd', '.', '\\n', '\\n', 'f', 'i', 'r', 's', 't', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'f', 'i', 'r', 's', 't', ',', ' ', 'y', 'o', 'u', ' ', 'k', 'n', 'o', 'w', ' ', 'c', 'a', 'i', 'u', 's', ' ', 'm', 'a', 'r', 'c', 'i', 'u', 's', ' ', 'i', 's', ' ', 'c', 'h', 'i', 'e', 'f', ' ', 'e', 'n', 'e', 'm', 'y', ' ', 't', 'o', ' ', 't', 'h', 'e', ' ', 'p', 'e', 'o', 'p', 'l', 'e', '.', '\\n', '\\n', 'a', 'l', 'l', ':', '\\n', 'w', 'e', ' ', 'k', 'n', 'o', 'w', \"'\", 't', ',', ' ', 'w', 'e', ' ', 'k', 'n', 'o', 'w', \"'\", 't', '.', '\\n', '\\n', 'f', 'i', 'r', 's', 't', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'l', 'e', 't', ' ', 'u', 's', ' ', 'k', 'i', 'l', 'l', ' ', 'h', 'i', 'm', ',', ' ', 'a', 'n', 'd', ' ', 'w', 'e', \"'\", 'l', 'l', ' ', 'h', 'a', 'v', 'e', ' ', 'c', 'o', 'r', 'n', ' ', 'a', 't', ' ', 'o', 'u', 'r', ' ', 'o', 'w', 'n', ' ', 'p', 'r', 'i', 'c', 'e', '.', '\\n', 'i', 's', \"'\", 't', ' ', 'a', ' ', 'v', 'e', 'r', 'd', 'i', 'c', 't', '?', '\\n', '\\n', 'a', 'l', 'l', ':', '\\n', 'n', 'o', ' ', 'm', 'o', 'r', 'e', ' ', 't', 'a', 'l', 'k', 'i', 'n', 'g', ' ', 'o', 'n', \"'\", 't', ';', ' ', 'l', 'e', 't', ' ', 'i', 't', ' ', 'b', 'e', ' ', 'd', 'o', 'n', 'e', ':', ' ', 'a', 'w', 'a', 'y', ',', ' ', 'a', 'w', 'a', 'y', '!', '\\n', '\\n', 's', 'e', 'c', 'o', 'n', 'd', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'o', 'n', 'e', ' ', 'w', 'o', 'r', 'd', ',', ' ', 'g', 'o', 'o', 'd', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', 's', '.', '\\n', '\\n', 'f', 'i', 'r', 's', 't', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'w', 'e', ' ', 'a', 'r', 'e', ' ', 'a', 'c', 'c', 'o', 'u', 'n', 't', 'e', 'd', ' ', 'p', 'o', 'o', 'r']\n"
     ]
    }
   ],
   "source": [
    "# TEST: Is your tokenized text the way you expected?\n",
    "tokenized_text = tokenize_text(raw_text[:500])\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Postprocessing\n",
    "\n",
    "We'll skip postprocessing since we don't have any special tokens we want to\n",
    "consider for our task here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode (Tokens → Integer IDs)\n",
    "\n",
    "We have `encode_text()` from our helper module that can encode our text based on\n",
    "our tokenization process from our created `tokenize_text()` function.\n",
    "\n",
    "This will also provide us with `character_mapping`, an object that we can use to\n",
    "map our tokens back and forth from strings to integer IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text, character_mapping = encode_text(raw_text, tokenize_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataset: 50,086 characters\n"
     ]
    }
   ],
   "source": [
    "n_tokens = character_mapping.n_tokens\n",
    "dataset_size = len(encoded_text)\n",
    "print(f'Size of dataset: {dataset_size:,} characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining sequence length that will be taken in at a time by our model\n",
    "sequence_length = 32 # Number of characters\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = ShakespeareDataset(encoded_text, sequence_length)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=False, # Ensure deterministic training\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model\n",
    "\n",
    "We'll provide a defined model today, but this could be a step that you would\n",
    "modify and experiment in other NLP projects you'll do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model to be trained and generate text with\n",
    "model = build_model(n_tokens)\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Text Generation\n",
    "\n",
    "The `generate_text_by_char()` function will use your tokenizer and NLP model to\n",
    "generate new text token-by-token (character-by-character in this case) by taking\n",
    "in the input text and token sampling parameters.\n",
    "\n",
    "We can use temperature and top-k sampling to adjust the \"creativeness\" of the\n",
    "generated text.\n",
    "\n",
    "We also pass in the `num_chars` parameter to tell the function how many tokens\n",
    "(characters in this case) to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_by_char(\n",
    "    input_str: str,\n",
    "    model,\n",
    "    token_mapping: TokenMapping = character_mapping,\n",
    "    num_chars: int = 100,\n",
    "    temperature: float = 1.0,\n",
    "    topk: int | None = None,\n",
    ") -> str:\n",
    "    # Uses your character-based tokenizer\n",
    "    tokenized_text: list[str] = tokenize_text(input_str)\n",
    "    # Generates token-by-token and creates a list of those tokens\n",
    "    generated_tokens = []\n",
    "    for _ in range(num_chars):\n",
    "        # Uses the input text and generated text (so far) to get next token\n",
    "        new_char = next_token(\n",
    "            tokenized_text=(tokenized_text + generated_tokens),\n",
    "            model=model,\n",
    "            token_mapping=token_mapping,\n",
    "            # Temperature & top-k sampling used in determining the next token\n",
    "            temperature=temperature,\n",
    "            topk=topk,\n",
    "            device=device,\n",
    "        )\n",
    "        generated_tokens.append(new_char)\n",
    "    # Returns input string plus the full generated string (of generated tokens)\n",
    "    full_text = ''.join(tokenized_text + generated_tokens)\n",
    "    return full_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "\n",
    "At this point, the model has not been trained so the code below will train the\n",
    "NLP model that will be used to generate new text.\n",
    "\n",
    "The model will take in the text data (broken by tokens by our character-based\n",
    "tokenizer) and attempt to predict the next token. Over time, the model should\n",
    "hopefully get better in predicting the next token (given the previous text).\n",
    "\n",
    "To help us visualize how the model is training, at the end of every epoch, we\n",
    "generate text using the `TEST_PHRASE` with the improving model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Loss: 2.5301671787572744\n",
      "[00m 28.0s (0 0.0) 2.1864]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to beelld bem\n",
      "ton thacd an,\n",
      "\n",
      "ickedr faratwuy ofne sutwot theceres'd to tholly id hore pefoma:\n",
      "arrim? sobo\n",
      "Epoch 2/25, Loss: 2.1812583873827998\n",
      "[00m 58.9s (1 4.0) 1.9870]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to bent fre'n weirtinin:\n",
      "and the whathy and the wowper, not to fortry ond rotse hon: the harey, the hice \n",
      "Epoch 3/25, Loss: 2.078537762584016\n",
      "[01m 26.3s (2 8.0) 1.8850]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be tas onen to prainn ther doww\n",
      "sirs move cocan upfle. her and not of\n",
      "coim noflenos thy thate hamsty, \n",
      "Epoch 4/25, Loss: 2.0192134959248307\n",
      "[01m 55.6s (3 12.0) 1.8197]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to beed ine him will pando soups thelanus:\n",
      "the bat hons ylomy on and keas ame mant;\n",
      "howtileas fate is lot\n",
      "Epoch 5/25, Loss: 1.9766919618978287\n",
      "[02m 26.2s (4 16.0) 1.7796]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to bem ase pory for thrame, ang thebbet's:\n",
      "wollor:\n",
      "your\n",
      "the comprake thaist, mary bt hader o sore him on \n",
      "Epoch 6/25, Loss: 1.9433630327828013\n",
      "[02m 58.9s (5 20.0) 1.7546]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to berieg hied;\n",
      "the thous offator\n",
      "on and as than beat thear, sututu th arch redpore on this citay\n",
      "frallek\n",
      "Epoch 7/25, Loss: 1.9164595753240128\n",
      "[03m 26.6s (6 24.0) 1.7339]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to bee soring ye notn:\n",
      "'t. mane dactay,\n",
      "and to pore.\n",
      "\n",
      "\n",
      "and thave on kuse.\n",
      "\n",
      "me popl.\n",
      "\n",
      "bofst estre wolabcom\n",
      "Epoch 8/25, Loss: 1.8943748301210495\n",
      "[03m 56.0s (7 28.000000000000004) 1.7176]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to besthe for yournesse tourwe valukanwo to is in and me the withik.\n",
      "\n",
      "for thably is reper to. that,\n",
      "whis \n",
      "Epoch 9/25, Loss: 1.8757073979027354\n",
      "[04m 25.9s (8 32.0) 1.7068]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be with lets use!\n",
      "my rome: you noth in af thoust to do nor's the than our yountt my ip, they, i he, to\n",
      "Epoch 10/25, Loss: 1.8597314305960562\n",
      "[04m 59.0s (9 36.0) 1.6993]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to becof erereed dece, ease checetius:\n",
      "opeavin congring mming cannjow alf\n",
      "oplees; ap athild? we henesns\n",
      "b\n",
      "Epoch 11/25, Loss: 1.8458594358005462\n",
      "[05m 26.0s (10 40.0) 1.6941]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be leo hunner thantuon his then in theix; your morienter all ar, thems,\n",
      "ther;'s; weerup if stoutle be \n",
      "Epoch 12/25, Loss: 1.8337552828529773\n",
      "[05m 58.8s (11 44.0) 1.6896]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be how of a, worditcu; this the's seen hum!\n",
      "\n",
      "pain.\n",
      "\n",
      "sive hetles not, then to tron prestreat the do son\n",
      "Epoch 13/25, Loss: 1.8230699228402525\n",
      "[06m 34.0s (12 48.0) 1.6847]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be\n",
      "cition; lite of their heelew have me to strueseas their beld for in to them rave the have ones, man\n",
      "Epoch 14/25, Loss: 1.8135556336789846\n",
      "[07m 5.5s (13 52.0) 1.6784]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be have our in'd of decerty'm worts in it of doflest but word thus the thle that uson.\n",
      "\n",
      "mary his in ot\n",
      "Epoch 15/25, Loss: 1.8050946429133796\n",
      "[07m 32.2s (14 56.00000000000001) 1.6712]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be him he wron!\n",
      "\n",
      "the peceny thoughty;\n",
      "weras, thowof, your det eart wea'lt,\n",
      "thein his of hing i\n",
      "hererea\n",
      "Epoch 16/25, Loss: 1.7974912706655435\n",
      "[08m 11.1s (15 60.0) 1.6645]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to betherbe pown: well stot, hoake wald mant o, ins probe, and heradan leter he cedour unous\n",
      "and at for m\n",
      "Epoch 17/25, Loss: 1.7906103176049912\n",
      "[08m 48.2s (16 64.0) 1.6590]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be that the lartius:\n",
      "and this evere look will\n",
      "for gat'th\n",
      "the\n",
      "twisintly\n",
      "\n",
      "ford to become!\n",
      "\n",
      "bure. netther\n",
      "Epoch 18/25, Loss: 1.7843517984445103\n",
      "[09m 24.7s (17 68.0) 1.6552]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to betieg whor the liaw sounder\n",
      "blents icent our laon to ancome, bech of he to powers and of not'ssare mu\n",
      "Epoch 19/25, Loss: 1.7786252254114365\n",
      "[09m 57.8s (18 72.0) 1.6521]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to beatrand prublentens of of hink, wither and to put's trot atutus:\n",
      "bath mening dims look! thalive star\n",
      "\n",
      "Epoch 20/25, Loss: 1.7733724317611597\n",
      "[10m 33.6s (19 76.0) 1.6488]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to benerurn spair hads lovent officenv you it.\n",
      "\n",
      "volumnius:\n",
      "i bod\n",
      "i and ranty dat me rechs grfied and lees\n",
      "Epoch 21/25, Loss: 1.7685356121855422\n",
      "[11m 23.6s (20 80.0) 1.6451]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be at is, for to herseld shangol;\n",
      "fach your s us, is\n",
      "litch stricius\n",
      "the getery bobry so hay kners are \n",
      "Epoch 22/25, Loss: 1.7640523532708994\n",
      "[12m 1.9s (21 84.0) 1.6418]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to beserfruponees and and as commane-ret, beag that,\n",
      "woll ast me, there blears offing,\n",
      "wheresterth marciu\n",
      "Epoch 23/25, Loss: 1.759862113684511\n",
      "[12m 30.2s (22 88.0) 1.6392]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be and wow honourive'; to him trov, and our;\n",
      "to most the would the way reemseluning my thevest him dit\n",
      "Epoch 24/25, Loss: 1.755975965265268\n",
      "[13m 6.2s (23 92.0) 1.6372]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to becopfirs\n",
      "mere\n",
      "sirging hopeegs'icicighly owed\n",
      "tikesses partiunt; 'tis foo, when yourus,\n",
      "burby fasty to\n",
      "Epoch 25/25, Loss: 1.7523777286846416\n",
      "[13m 33.0s (24 96.0) 1.6360]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to bere rose yamas grathtreced:\n",
      "we yet pee's not tere\n",
      "setter's upon'th\n",
      "of tlrame for at for them hith\n",
      "and\n"
     ]
    }
   ],
   "source": [
    "TEST_PHRASE = 'To be or not to be'\n",
    "# Use more epochs if not CPU device\n",
    "epochs = 5 if device == 'cpu' else 25\n",
    "\n",
    "start = start_time()\n",
    "for epoch in range(epochs):\n",
    "    # Set model into \"training mode\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch.to(device))\n",
    "        loss = criterion(output.transpose(1, 2), y_batch.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_loader)}')\n",
    "    print(f'[{time_since(start)} ({epoch} {epoch / epochs * 100}) {loss:.4f}]')\n",
    "    print('-'*72)\n",
    "    gen_output = generate_text_by_char(\n",
    "        input_str=TEST_PHRASE,\n",
    "        model=model,\n",
    "        num_chars=100,\n",
    "    )\n",
    "    print(gen_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Text\n",
    "\n",
    "Now that the model has been trained, go ahead and observe how it performs!\n",
    "\n",
    "Try adjusting the different sampling methods using the `temperature` and `topk`\n",
    "parameters on the same input string to see the differences.\n",
    "\n",
    "You might also try different phrases as well as how many tokens (`num_chars`) to\n",
    "generate and observe how it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to be or not to berner, is fur, withs\n",
      "forttayitel-fide of then:\n",
      "apee, thand. to esses\n",
      "acke the petter,\n",
      "\n",
      "marigh the sec\n"
     ]
    }
   ],
   "source": [
    "output = generate_text_by_char(\n",
    "    input_str='To be or not to be',\n",
    "    model=model,\n",
    "    num_chars=100,\n",
    "    temperature=1.0,\n",
    "    topk=None,\n",
    ")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subword Text Generation\n",
    "\n",
    "The next model you'll build will use subword-tokenization instead of \n",
    "characters-based token to train a model and ultimately generate new text\n",
    "token-by-token.\n",
    "\n",
    "Although this could be done by creating your own tokenizer, you'll use\n",
    "Hugging Face to use a pretrained tokenize to tokenizer the data.\n",
    "\n",
    "After training the model with subword tokens, \n",
    "the model will take in a new string, token-by-token, and then generate a new\n",
    "token (subword).\n",
    "The model will continue producing new subword tokens based on the input text\n",
    "and already produced tokens until a set number of tokens have been generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Text into Integer Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing a Tokenizer\n",
    "\n",
    "> NOTE:\n",
    "> \n",
    "> You can load another model outside of these choices but the model\n",
    "> will have to be downloaded and may or may not be effective.\n",
    ">\n",
    "> If you'd like to explore more, here's a link to you might want to start with\n",
    "> of different available pretrained models on Hugging Face:\n",
    "> https://huggingface.co/models?pipeline_tag=text-generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# COMPLETE: Choose a pretrained tokenizer to use:\n",
    "\n",
    "# Docs: https://huggingface.co/xlm-roberta-base\n",
    "# model_name = 'xlm-roberta-base'\n",
    "# DOCS: https://huggingface.co/bert-base-cased\n",
    "# model_name = 'bert-base-cased'\n",
    "# DOCS: https://huggingface.co/bert-base-uncased \n",
    "model_name = 'bert-base-uncased'\n",
    "\n",
    "my_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode (Tokens → Integer IDs)\n",
    "\n",
    "We have `encode_text_from_tokenizer()` from our helper module that can encode\n",
    "our text based on our tokenization process from our tokenizer `my_tokenizer`.\n",
    "\n",
    "This will also provide us with `token_mapping`, an object that we can use to\n",
    "map our tokens back and forth from strings to integer IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text, token_mapping = encode_text_from_tokenizer(\n",
    "    text=raw_text,\n",
    "    tokenizer=my_tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataset: 13,139 tokens\n"
     ]
    }
   ],
   "source": [
    "n_tokens = token_mapping.n_tokens\n",
    "dataset_size = len(encoded_text)\n",
    "print(f'Size of dataset: {dataset_size:,} tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining sequence length that will be taken in at a time by our model\n",
    "sequence_length = 32 # Number of tokens\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = ShakespeareDataset(encoded_text, sequence_length)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=False, # Ensure deterministic training\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model\n",
    "\n",
    "We'll provide a defined model today, but this could be a step that you would\n",
    "modify and experiment in other NLP projects you'll do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model to be trained and generate text with\n",
    "model = build_model(n_tokens)\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Text Generation\n",
    "\n",
    "The `generate_text_by_subword()` function will use your chosen tokenizer and the\n",
    "NLP model to generate new text token-by-token (subwords in this case) by taking\n",
    "in the input text and token sampling parameters.\n",
    "\n",
    "We can use temperature and top-k sampling to adjust the \"creativeness\" of the\n",
    "generated text.\n",
    "\n",
    "We also pass in the `num_tokens` parameter to tell the function how many\n",
    "(subword)tokens to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_by_subword(\n",
    "    input_str: str,\n",
    "    model,\n",
    "    token_mapping: TokenMapping = token_mapping,\n",
    "    tokenizer = my_tokenizer,\n",
    "    num_tokens: int = 100,\n",
    "    temperature: float = 1.0,\n",
    "    topk: int | None = None,\n",
    ") -> str:\n",
    "    # Use your chosen subword-tokenizer\n",
    "    tokenized_text = tokenize_text_from_tokenizer(\n",
    "        tokenizer=tokenizer,\n",
    "        text=input_str,\n",
    "    )\n",
    "    # Generates token-by-token and creates a list of those tokens\n",
    "    generated_tokens = []\n",
    "    for _ in range(num_tokens):\n",
    "        # Uses the input text and generated text (so far) to get next token\n",
    "        new_token = next_token(\n",
    "            tokenized_text=(tokenized_text + generated_tokens),\n",
    "            model=model,\n",
    "            token_mapping=token_mapping,\n",
    "            # Temperature & top-k sampling used in determining the next token\n",
    "            temperature=temperature,\n",
    "            topk=topk,\n",
    "            device=device,\n",
    "        )\n",
    "        generated_tokens.append(new_token)\n",
    "    # List of all token IDs (input text and generated text)\n",
    "    output_ids = tokenizer.convert_tokens_to_ids(\n",
    "        tokenized_text + generated_tokens\n",
    "    )\n",
    "    # Returns input string plus the full generated string from list of token IDs\n",
    "    full_text = tokenizer.decode(output_ids)\n",
    "    return full_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "\n",
    "At this point, the model has not been trained so the code below will train the\n",
    "NLP model that will be used to generate new text.\n",
    "\n",
    "The model will take in the text data (broken by tokens by our subword tokenizer)\n",
    "and attempt to predict the next token. Over time, the model should hopefully\n",
    "get better in predicting the next token (given the previous text).\n",
    "\n",
    "To help us visualize how the model is training, at the end of every epoch, we\n",
    "generate text using the `TEST_PHRASE` with the improving model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Loss: 6.530676760324618\n",
      "[00m 24.7s (0 0.0) 5.4555]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be freedoms ; no. speak go e spare himces serpent enough little youth that art, to bed, br fool has -, he, body marchled\n",
      "Epoch 2/25, Loss: 5.858244466781616\n",
      "[00m 50.1s (1 4.0) 4.9384]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be act : he ' the fit like, were the ridgeses llter encounter speak : of be ' theward, and cr leave another you cove repetition\n",
      "Epoch 3/25, Loss: 5.580164302267679\n",
      "[01m 11.6s (2 8.0) 4.6046]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to beenen thus i ladies a marcius : rome, and earnest for there ' s and ' em there loved will half belly and envy my maderio\n",
      "Epoch 4/25, Loss: 5.328533220872647\n",
      "[01m 33.1s (3 12.0) 4.3840]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be ; the soldier thought blood fell sha yourouss the can in, and thelanus bynts a lamb,,lanus : shall ; men\n",
      "Epoch 5/25, Loss: 5.119738193837608\n",
      "[01m 54.4s (4 16.0) 4.2123]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be answer ; well of in theirs : where thousand honourable change : and where he gone - an them my thigh the controversy. come ava opinion a granted\n",
      "Epoch 6/25, Loss: 4.943046848948409\n",
      "[02m 15.1s (5 20.0) 4.0695]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to beiest home - heree we ink minister of struck tribune af way thater.? for us wish rash to thoughts. marcius him? corio\n",
      "Epoch 7/25, Loss: 4.789231310821161\n",
      "[02m 38.8s (6 24.0) 3.9466]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be held and ario aufidi and your gains your sha ears report ; and but careless tots! st follows vol come wouldon you. fall in\n",
      "Epoch 8/25, Loss: 4.652143506887483\n",
      "[03m 9.7s (7 28.000000000000004) 3.8379]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to beh sl dr their out chain the gods say! upon? o the other say we the suit and takes herfulused we nor hear in '\n",
      "Epoch 9/25, Loss: 4.528026088272653\n",
      "[03m 31.5s (8 32.0) 3.7409]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be on like more must i sick thend their lady between rome, ha fast forth? sicinius : - ' d from to stride, come he\n",
      "Epoch 10/25, Loss: 4.413991845526346\n",
      "[03m 54.7s (9 36.0) 3.6504]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be put ' t a officer e them ; and notc will rome, thou shall be partly against flat which ; and pre. first citizen : see this\n",
      "Epoch 11/25, Loss: 4.308145373042037\n",
      "[04m 18.1s (10 40.0) 3.5666]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be ta be bath marcius? brutus :! butt apply this.st to done, but you ' laugh notrong to virgil to : and his\n",
      "Epoch 12/25, Loss: 4.209631301135552\n",
      "[04m 45.0s (11 44.0) 3.4881]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be hastesome they you, my goo - - die his human like to know one thy reel auger in coriolanus should held to the city\n",
      "Epoch 13/25, Loss: 4.117172505797409\n",
      "[05m 11.8s (12 48.0) 3.4146]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be safely?, you are and a youthherly, for their liking petition his cracking.st with him, had in fever. marcius : i\n",
      "Epoch 14/25, Loss: 4.029876803770298\n",
      "[05m 34.8s (13 52.0) 3.3457]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be prevailing hi my fright - - what. valeria : though trusts be myrb of our the which gods not a common cushionaves he loved s be\n",
      "Epoch 15/25, Loss: 3.9469910243662394\n",
      "[06m 7.9s (14 56.00000000000001) 3.2808]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be incensed : we whether nurse people! and abundance in whom would stand ' door, that have at, and yettangled attain in compound, and in\n",
      "Epoch 16/25, Loss: 3.8679350050484262\n",
      "[06m 32.4s (15 60.0) 3.2196]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be become in the nobility via is the neck, and wonher my prevailing and make forth and best. volumnia : but o your see.\n",
      "Epoch 17/25, Loss: 3.79252318638127\n",
      "[06m 53.5s (16 64.0) 3.1618]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be wheel mali no herd. worthy lartius : a word. first citizen for the controversy of itself : and he doubt lift asst and titus ri\n",
      "Epoch 18/25, Loss: 3.720735192298889\n",
      "[07m 23.0s (17 68.0) 3.1070]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be so thing is the word of what are had coming heart. volumnia : we do give him won wonder him see cl glad me whom conity\n",
      "Epoch 19/25, Loss: 3.6523144204442093\n",
      "[07m 46.2s (18 72.0) 3.0545]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be lie pen sights? volumnia : why, thy grim mat elders that would : appetite the silence, and endt to particularisehelm not hang\n",
      "Epoch 20/25, Loss: 3.5870098177979632\n",
      "[08m 5.8s (19 76.0) 3.0044]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be restrain the war my general with rushes at hate : o. sicinius : i ' theural aside, or begin ; enemy - ' tribune to\n",
      "Epoch 21/25, Loss: 3.524647403926384\n",
      "[08m 25.1s (20 80.0) 2.9567]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be my fancy : and carry he will go shall be made him alone, these baseled thoroughly earnest myane, let him cannot cal things for the picture\n",
      "Epoch 22/25, Loss: 3.465125017631345\n",
      "[08m 44.7s (21 84.0) 2.9110]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be nor laugh them, it that stone in the people in, for the present than alone, were she delay it with burdens. sicinius :\n",
      "Epoch 23/25, Loss: 3.408431877159491\n",
      "[09m 3.6s (22 88.0) 2.8677]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be otherwise ' fore us to theie, mark mad bold strike daughter, the volsces shows be none! all should done their counsel thatrs! '\n",
      "Epoch 24/25, Loss: 3.3543773203361327\n",
      "[09m 21.2s (23 92.0) 2.8267]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to besee complexion time home on lartius : we do the coli. marcius : most does praise not his war that i would they me of our\n",
      "Epoch 25/25, Loss: 3.3026665629410163\n",
      "[09m 38.6s (24 96.0) 2.7884]\n",
      "------------------------------------------------------------------------\n",
      "to be or not to be pin beat against the wind of hand, that lamb up, and and so as disdain us, that darkship. first officer : this way in neither\n"
     ]
    }
   ],
   "source": [
    "TEST_PHRASE = 'To be or not to be'\n",
    "# Use more epochs if not CPU device\n",
    "epochs = 5 if device == 'cpu' else 25\n",
    "\n",
    "start = start_time()\n",
    "for epoch in range(epochs):\n",
    "    # Set model into \"training mode\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch.to(device))\n",
    "        loss = criterion(output.transpose(1, 2), y_batch.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_loader)}')\n",
    "    print(f'[{time_since(start)} ({epoch} {epoch / epochs * 100}) {loss:.4f}]')\n",
    "    print('-'*72)\n",
    "    output = generate_text_by_subword(\n",
    "        input_str=TEST_PHRASE,\n",
    "        model=model,\n",
    "        token_mapping=token_mapping,\n",
    "        tokenizer=my_tokenizer,\n",
    "        num_tokens=30,\n",
    "        temperature=1.0,\n",
    "    )\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Text\n",
    "\n",
    "Now that the model has been trained, go ahead and observe how it performs!\n",
    "\n",
    "Try adjusting the different sampling methods using the `temperature` and `topk`\n",
    "parameters on the same input string to see the differences.\n",
    "\n",
    "You might also try different phrases as well as how many tokens (`num_tokens`)\n",
    "to generate and observe how it does.\n",
    "\n",
    "------------\n",
    "\n",
    "Consider how this model differs from the results from the text generation using\n",
    "the character-based tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to be or not to be crept again of geese very imperfect ; for will a hundred helps time it that that upon : the volsces man! brutus : i was and grave\n"
     ]
    }
   ],
   "source": [
    "output = generate_text_by_subword(\n",
    "        input_str='To be or not to be',\n",
    "        model=model,\n",
    "        token_mapping=token_mapping,\n",
    "        tokenizer=my_tokenizer,\n",
    "        num_tokens=30,\n",
    "        temperature=1.5,\n",
    "        topk=100,\n",
    "    )\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
